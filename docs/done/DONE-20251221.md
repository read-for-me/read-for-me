# 개발 완료 이력 - 2025-12-21

> Phase 2 크롤링 아키텍처 완료: GeekNews/Medium 크롤러, 팩토리 패턴, API 엔드포인트, 통합 테스트 완료. Phase 3 Vertex AI 인증 준비.

---

## Phase 2: 크롤링 및 ETL 아키텍처 (완료)

> **목표**: 다양한 소스에 대응하며 고품질의 정제된 텍스트를 출력하는 크롤러 엔진 구축.
> **지원 플랫폼 (MVP)**: GeekNews Article, Medium Article
> **완료일**: 2025-12-21

### Phase 2 완료 체크리스트

- [x] `schemas.py`: Pydantic 데이터 모델 정의 완료 (`CrawlRequest`, `ArticleMetadata`, `CrawledArticle`, `CleanedArticle`)
- [x] `base.py`: 추상 기본 크롤러 (`BaseCrawler`) 및 텍스트 추출기 (`BaseTextExtractor`) 구현 완료
- [x] `geeknews.py`: GeekNews Article 크롤러 구현 완료
- [x] `medium.py`: Medium Article 크롤러 구현 완료
- [x] `factory.py`: URL 기반 크롤러 팩토리 구현 완료 (`CrawlerFactory`, `UnsupportedURLError`)
- [x] `crawl.py`: API 엔드포인트 구현 완료 (POST `/crawl`, GET `/supported-domains`, GET `/check-support`)
- [x] **크롤링 결과 JSON 저장**: `backend/data/crawled/{platform}/` 경로에 자동 저장
- [x] **통합 테스트 완료**: GeekNews, Medium 실제 URL 크롤링 검증
- 참고 코드: `reference/data_crawling/src/` 디렉토리

---

### GeekNews 크롤러 구현

- **GeekNewsCrawler 구현** (`backend/app/services/crawlers/geeknews.py`):
  - `BaseCrawler`를 상속한 GeekNews 전용 크롤러
  - **URL 패턴**: `https://news.hada.io/topic?id=XXXXX`
  - **추상 메서드 구현**:
    - `validate_url()`: URL 정규식 패턴 검증
    - `extract()`: 비동기 크롤링 파이프라인 (validate → fetch → parse → extract)
    - `_parse_content()`: BeautifulSoup에서 구조화된 데이터 추출
  - **Private Helper 메서드**:
    - `_extract_title()`: `.topictitle h1` 또는 `.topictitle a.ud`에서 제목 추출
    - `_extract_original_url()`: 외부 원본 링크 추출 (GeekNews 내부 링크 제외)
    - `_extract_meta_info()`: 작성자, 포인트, 게시 시간 추출
    - `_extract_main_content()`: `.topic_contents`에서 본문 추출
    - `_format_content()`: ul/li → bullet point, h1-h6 → # 헤더, blockquote → > 인용구
    - `_extract_comments()`: 댓글 추출 (depth 구조 보존)
    - `_extract_topic_id()`: URL에서 토픽 ID 추출
    - `_build_content()`: 추출된 데이터를 하나의 콘텐츠 문자열로 조합
  - **특징**:
    - 비동기(async/await) 지원으로 FastAPI와 완벽 호환
    - `include_comments` 옵션으로 댓글 포함 여부 선택 가능
    - `secondary_urls`에 원본 외부 링크 포함
- **패키지 export 업데이트** (`backend/app/services/crawlers/__init__.py`):
  - `GeekNewsCrawler` 클래스 export 추가

---

### Medium 크롤러 구현 (Phase 2 완료)

- **MediumCrawler 구현** (`backend/app/services/crawlers/medium.py`):
  - `BaseCrawler`를 상속한 Medium 전용 크롤러
  - **Freedium 미러 사이트 활용** (`https://freedium.cfd/{medium_url}`):
    - 봇 탐지 우회
    - 페이월 콘텐츠 접근 가능
    - 안정적인 HTML 구조
  - **URL 패턴**: `https://medium.com/@username/...`, `https://medium.com/publication/...`, 서브도메인
  - **추상 메서드 구현**:
    - `validate_url()`: URL 정규식 패턴 검증 (다중 패턴 지원)
    - `extract()`: 비동기 크롤링 파이프라인 (Freedium 우선 → 실패 시 원본 Medium fallback)
    - `_parse_content()`: BeautifulSoup에서 구조화된 데이터 추출 (Freedium 파싱 로직 사용)
  - **URL 변환 메서드**:
    - `_convert_to_freedium_url()`: Medium URL을 Freedium URL로 변환
    - `_extract_original_url()`: Freedium URL에서 원본 Medium URL 추출
  - **Freedium 파싱 메서드**:
    - `_parse_freedium_content()`: Freedium HTML에서 아티클 데이터 추출
    - `_extract_freedium_title()`: h1 또는 title 태그에서 제목 추출
    - `_extract_freedium_metadata()`: 작성자, 부제목, 날짜 추출
    - `_extract_freedium_body()`: main/article/content 내 본문 마크다운 포맷팅
  - **Medium 원본 파싱 메서드 (Fallback)**:
    - `_parse_medium_content()`: 원본 Medium HTML 파싱 (JavaScript 렌더링 제한)
    - `_extract_medium_metadata()`: `data-testid` 속성으로 메타데이터 추출
    - `_extract_medium_body()`: section/article 내 본문 추출
  - **공통 유틸리티**:
    - `_format_tag()`: 태그를 마크다운 형식으로 변환 (h1-h6, p, pre, blockquote, ul/ol, figure/img)
    - `_build_content()`: 메타정보 + 본문 + 태그를 하나의 콘텐츠 문자열로 조합
  - **생성자 옵션**:
    - `use_freedium: bool = True`: Freedium 미러 사이트 사용 여부
    - `request_delay: float = 0.5`: Rate limiting 방지를 위한 요청 지연 (초)
  - **특징**:
    - 비동기(async/await) 지원으로 FastAPI와 완벽 호환
    - Freedium 실패 시 원본 Medium URL로 자동 fallback
    - 코드블록, 인용구, 리스트, 이미지 캡션 마크다운 변환
- **MediumTextExtractor 클래스**: Medium/Freedium 특화 노이즈 제거 유틸리티
- **schemas.py 업데이트**: `ArticleMetadata`에 `subtitle` 필드 추가
- **패키지 export 업데이트** (`backend/app/services/crawlers/__init__.py`):
  - `MediumCrawler` 클래스 export 추가

---

### 크롤러 팩토리 구현

- **CrawlerFactory 구현** (`backend/app/services/crawlers/factory.py`):
  - URL 도메인 기반 크롤러 선택 팩토리
  - **주요 메서드**:
    - `get_crawler(url, **kwargs)`: URL에서 도메인 추출 후 적절한 크롤러 인스턴스 반환
    - `get_supported_domains()`: 지원하는 도메인 패턴 목록 반환
    - `is_supported(url)`: URL 지원 여부 확인
    - `register_crawler(domain, cls)`: 런타임 크롤러 동적 등록
  - **특징**:
    - 서브도메인 지원 (`towardsdatascience.medium.com` → `MediumCrawler`)
    - `www.` 접두사 자동 제거
    - kwargs 전달 지원 (`include_comments=True` 등)
- **UnsupportedURLError 예외 클래스**: 지원하지 않는 URL에 대한 상세 정보 제공
- **패키지 export 업데이트** (`backend/app/services/crawlers/__init__.py`):
  - `CrawlerFactory`, `UnsupportedURLError` 클래스 export 추가

---

### 크롤링 API 엔드포인트 구현 (Phase 2 완료)

- **Crawl API 구현** (`backend/app/api/v1/crawl.py`):
  - `POST /api/v1/crawl`: URL 크롤링 후 정제된 아티클 반환
  - `GET /api/v1/crawl/supported-domains`: 지원 도메인 목록 반환
  - `GET /api/v1/crawl/check-support`: URL 지원 여부 확인
- **main.py 라우터 등록**: `app.include_router(crawl.router, prefix=settings.API_V1_STR)`
- **api/v1/__init__.py 업데이트**: crawl 모듈 export 추가

---

### Phase 2 통합 테스트 완료

- **테스트 수행**: 실제 URL로 크롤링 동작 검증 완료
  - GeekNews: `https://news.hada.io/topic?id=24400` ✅
  - Medium: `https://medium.com/data-science-collective/how-to-use-deepseek-ocr-and-docling-for-pdf-parsing-ae00a88e10e7` ✅
  - 에러 처리: `UnsupportedURLError` 정상 발동 ✅
- **버그 수정**:
  - `schemas.py`: 순환 import 문제 해결 (지연 import 적용)
  - `base.py`: httpx 인코딩 처리 수정 (`apparent_encoding` → 자동 처리)
  - `pyproject.toml`: `loguru` 의존성 추가

---

### 크롤링 결과 JSON 저장 기능 추가

- **저장 기능 구현** (`backend/app/api/v1/crawl.py`):
  - `save_crawl_result()`: 크롤링 결과를 JSON 파일로 저장하는 비동기 함수
  - `_extract_article_id()`: URL에서 아티클 ID/슬러그 추출 (플랫폼별 처리)
  - `crawl_url()` 엔드포인트에서 저장 로직 호출 (실패해도 응답은 반환)
- **저장 경로**: `backend/data/crawled/{platform}/{article_id}_{timestamp}.json`
  - GeekNews: `topic_24400_2025-12-21T14-30-00.json`
  - Medium: `how-to-use-deepseek-ocr_2025-12-21T14-35-00.json`
- **`.gitignore` 업데이트** (`backend/.gitignore`):
  - `data/` 디렉토리 무시 설정 추가
  - `.env` 파일 무시 설정 추가
- **저장되는 JSON 구조**:
  ```json
  {
    "title": "아티클 제목",
    "cleaned_content": "정제된 본문...",
    "preview_text": "미리보기 텍스트 (300자)...",
    "url": "https://...",
    "platform": "geeknews | medium",
    "crawled_at": "2025-12-21T14:30:00.123456+00:00",
    "metadata": { ... }
  }
  ```

---

## Phase 3: AI 서비스 구현 (준비)

### Phase 3 준비 (Vertex AI 인증)

- [x] Google VertexAI API 연동
  - [x] Google Cloud Service-Account Key 생성 및 인증 자동화 완료

```python
import os
from google.cloud import aiplatform

# 1. 환경 변수 강제 할당
key_path = os.path.expanduser("~/readforme-key.json")
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = key_path

# 2. Vertex AI 초기화
aiplatform.init(
    project="gen-lang-client-0039052673",
    location="us-central1" # 또는 'asia-northeast3' (서울)
)

# 이제 인증 없이 서비스 사용 가능
```

---

## 설계 결정 기록 (ADR)

### ADR-001: HTTP 클라이언트로 `httpx` 선택

| 항목     | 내용                                                                                                    |
| -------- | ------------------------------------------------------------------------------------------------------- |
| **결정** | `requests` 대신 `httpx` 사용                                                                            |
| **이유** | FastAPI의 비동기(async/await) 패턴과 완벽 호환. `requests`는 동기 전용이라 `run_in_executor` 래핑 필요. |
| **대안** | `aiohttp` - 기능은 비슷하나 API가 복잡하고, httpx가 requests와 유사한 인터페이스 제공                   |
| **결과** | 크롤러의 `fetch_html()` 메서드가 `async def`로 구현되어 FastAPI 엔드포인트에서 직접 호출 가능           |

### ADR-002: Medium 크롤링에 Freedium 미러 사이트 활용

| 항목         | 내용                                                                                                                                   |
| ------------ | -------------------------------------------------------------------------------------------------------------------------------------- |
| **결정**     | Medium 직접 크롤링 대신 Freedium(`https://freedium.cfd`) 미러 사이트 우선 사용                                                         |
| **이유**     | 1. Medium의 봇 탐지 회피 (Cloudflare, JavaScript 렌더링 필수) <br> 2. 페이월 콘텐츠 접근 가능 <br> 3. 안정적이고 예측 가능한 HTML 구조 |
| **대안**     | Selenium/Playwright - 헤드리스 브라우저 사용. 리소스 소모 크고 속도 느림.                                                              |
| **Fallback** | Freedium 실패 시 원본 Medium URL로 자동 전환 (`use_freedium=False` 옵션)                                                               |
| **리스크**   | Freedium 서비스 종료/변경 시 크롤러 수정 필요. Fallback 로직으로 대비.                                                                 |

### ADR-003: 크롤러 팩토리 패턴 적용

| 항목     | 내용                                                                                                                                                      |
| -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **결정** | URL 도메인 기반 크롤러 자동 선택 팩토리 (`CrawlerFactory`) 구현                                                                                           |
| **이유** | 1. API 엔드포인트에서 URL만 받으면 적절한 크롤러 자동 선택 <br> 2. 새 플랫폼 추가 시 `_crawlers` 딕셔너리에 한 줄만 추가 <br> 3. 확장성과 유지보수성 확보 |
| **구조** | `CrawlerFactory.get_crawler(url)` → 도메인 추출 → 크롤러 클래스 매핑 → 인스턴스 반환                                                                      |
| **확장** | `register_crawler(domain, cls)` 메서드로 런타임 동적 등록 가능                                                                                            |

### ADR-004: Python 패키지 관리자로 `uv` 선택

| 항목          | 내용                                                                                                                                    |
| ------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **결정**      | `pip` + `venv` 대신 `uv` 사용                                                                                                           |
| **이유**      | 1. Rust로 작성되어 10~100배 빠른 의존성 해결 <br> 2. `uv.lock`으로 재현 가능한 빌드 보장 <br> 3. `uv run` 명령으로 가상환경 자동 활성화 |
| **실행 방법** | `cd backend && uv run uvicorn app.main:app --reload`                                                                                    |

### ADR-005: 추상 기본 크롤러 (ABC) 설계

| 항목     | 내용                                                                                                                                                                                  |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **결정** | `BaseCrawler` ABC를 만들어 모든 크롤러가 상속하도록 설계                                                                                                                              |
| **이유** | 1. 공통 로직(fetch, parse, OG 메타 추출) 재사용 <br> 2. 추상 메서드로 플랫폼별 구현 강제 (`validate_url`, `extract`, `_parse_content`) <br> 3. 일관된 인터페이스로 팩토리 패턴과 호환 |
| **구조** | `BaseCrawler` → `GeekNewsCrawler`, `MediumCrawler` 상속                                                                                                                               |

### ADR-006: 콘텐츠 마크다운 포맷팅

| 항목          | 내용                                                                                                                             |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **결정**      | 크롤링된 HTML을 마크다운 형식으로 변환하여 저장                                                                                  |
| **이유**      | 1. LLM(Gemini)이 마크다운을 잘 이해함 <br> 2. 구조화된 텍스트로 요약/TTS 대본 생성 품질 향상 <br> 3. 사람이 읽기에도 가독성 좋음 |
| **변환 규칙** | `<h1-h6>` → `#~######`, `<ul>/<li>` → `• bullet`, `<blockquote>` → `> 인용구`, `<pre>` → \`\`\`코드블록\`\`\`                    |

### ADR-007: 크롤링 결과 로컬 JSON 저장

| 항목            | 내용                                                                                                                                   |
| --------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| **결정**        | 크롤링 결과를 `backend/data/crawled/{platform}/` 경로에 JSON 파일로 저장                                                               |
| **이유**        | 1. 개발/디버깅 시 실제 크롤링 결과 확인 용이 <br> 2. 동일 URL 재크롤링 시 캐시로 활용 가능 (추후) <br> 3. 데이터 분석/품질 검증에 활용 |
| **파일명 규칙** | `{article_id}_{timestamp}.json` (예: `topic_24400_2025-12-21T14-30-00.json`)                                                           |
| **Git 제외**    | `backend/.gitignore`에 `data/` 추가                                                                                                    |

---

## Phase 2 상세 레퍼런스

> 크롤러 구현 시 참고할 상세 스키마, 클래스 시그니처, 사용법 예시입니다.

### 디렉토리 구조

```
backend/app/
├── services/
│   └── crawlers/
│       ├── __init__.py       # 공개 API export
│       ├── schemas.py        # Pydantic 데이터 모델
│       ├── base.py           # BaseCrawler ABC + BaseTextExtractor
│       ├── geeknews.py       # GeekNewsCrawler
│       ├── medium.py         # MediumCrawler
│       └── factory.py        # CrawlerFactory (URL → 크롤러 매핑)
├── api/v1/
│   └── crawl.py              # POST /api/v1/crawl 엔드포인트
```

### 아키텍처 흐름

```
[POST /api/v1/crawl]
       │
       ▼
[CrawlerFactory] ─── URL 도메인 분석 ──► 적절한 크롤러 선택
       │
       ├─► news.hada.io  → GeekNewsCrawler
       └─► medium.com    → MediumCrawler
               │
               ▼
       [BaseCrawler.extract()]
               │
               ▼
       [CrawledArticle] → ETL → [CleanedArticle]
               │
               ▼
         API Response
```

---

### 2-1. 데이터 스키마 (`schemas.py`)

- 파일 위치: `backend/app/services/crawlers/schemas.py`

**구현된 스키마:**

| 클래스            | 역할                                                                       |
| ----------------- | -------------------------------------------------------------------------- |
| `CrawlRequest`    | API 요청 스키마 (url: HttpUrl)                                             |
| `ArticleMetadata` | OG 태그, 작성자, 게시일, 플랫폼별 확장 필드                                |
| `CrawledArticle`  | 크롤링 원본 결과 (title, content, url, platform, metadata, secondary_urls) |
| `CleanedArticle`  | ETL 후 정제된 결과 - API 응답용 (preview_text 포함)                        |

---

### 2-2. 추상 기본 크롤러 (`base.py`)

- 파일 위치: `backend/app/services/crawlers/base.py`
- 참고: `reference/data_crawling/src/base_crawler.py`

**구현된 클래스:**

| 클래스              | 역할                                                                                |
| ------------------- | ----------------------------------------------------------------------------------- |
| `BaseTextExtractor` | 텍스트 정제 유틸리티 (clean_text, remove_noise_elements, extract_text_from_element) |
| `BaseCrawler`       | 모든 크롤러의 추상 기본 클래스 (ABC)                                                |

**`BaseTextExtractor` 메서드:**

| 메서드                                   | 설명                                                        |
| ---------------------------------------- | ----------------------------------------------------------- |
| `clean_text(text)`                       | 공백/줄바꿈 정규화 (3줄↑ → 2줄, 탭/연속공백 → 스페이스 1개) |
| `remove_noise_elements(soup, selectors)` | 광고, 네비게이션 등 노이즈 요소 제거                        |
| `extract_text_from_element(element)`     | BeautifulSoup 요소에서 텍스트 추출                          |

**`BaseCrawler` 메서드:**

```python
class BaseCrawler(ABC):
    """모든 크롤러의 추상 기본 클래스"""

    platform_name: str = "base"
    DEFAULT_HEADERS: dict  # User-Agent, Accept 등

    # --- 공통 메서드 (구현됨) ---
    async def fetch_html(self, url: str) -> str | None:
        """httpx를 사용한 비동기 HTML 요청"""

    def parse_html(self, html: str) -> BeautifulSoup:
        """HTML → BeautifulSoup 파싱"""

    def extract_og_meta(self, soup: BeautifulSoup) -> dict:
        """OG 메타 태그 추출"""

    def _build_metadata(self, og_meta: dict, **extra_fields) -> ArticleMetadata:
        """OG 메타 정보와 추가 필드를 결합하여 ArticleMetadata 생성"""

    # --- 추상 메서드 (하위 클래스에서 구현 필수) ---
    @abstractmethod
    def validate_url(self, url: str) -> bool:
        """URL이 해당 플랫폼에 유효한지 검증"""

    @abstractmethod
    async def extract(self, url: str) -> CrawledArticle | None:
        """URL에서 콘텐츠 추출 (1차 페이지)"""

    @abstractmethod
    def _parse_content(self, soup: BeautifulSoup, url: str) -> CrawledArticle | None:
        """BeautifulSoup에서 구조화된 데이터 추출"""

    # --- 2차 URL 크롤링 (stub - 추후 구현) ---
    def extract_secondary_urls(self, soup: BeautifulSoup) -> list[str]:
        """2차 URL 목록 추출 - 기본: 빈 리스트"""
        return []

    async def crawl_secondary(self, urls: list[str]) -> list[CrawledArticle]:
        """2차 URL 크롤링 - NotImplementedError"""
        raise NotImplementedError("Secondary URL crawling is not yet implemented")
```

---

### 2-3. GeekNews 크롤러 (`geeknews.py`)

- 파일 위치: `backend/app/services/crawlers/geeknews.py`
- 참고: `reference/data_crawling/src/geeknews_article_crawler.py`, `geeknews_base.py`

**URL 패턴:**

```
https://news.hada.io/topic?id=XXXXX
예: https://news.hada.io/topic?id=24268
```

**추출 항목:**

| 항목           | 선택자/방법                              | 비고                                     |
| -------------- | ---------------------------------------- | ---------------------------------------- |
| 제목           | `.topictitle h1` 또는 `.topictitle a.ud` | fallback: `<title>` 태그                 |
| 원본 외부 링크 | `.topictitle a.ud[href]`                 | GeekNews 내부 링크 제외                  |
| 작성자         | `.topicinfo a[href*='/user']`            |                                          |
| 포인트         | `.topicinfo` 내 `(\d+)P` 패턴            |                                          |
| 게시 시간      | `.topicinfo span[title]`                 | ISO 형식, fallback: 상대시간             |
| 본문           | `.topic_contents`                        | ul/li → bullet point 변환                |
| 댓글 (선택)    | `.comment_row`                           | depth 구조 보존, `include_comments` 옵션 |
| 토픽 ID        | URL query param `id`                     | 메타데이터용                             |

**구현된 클래스:**

```python
class GeekNewsCrawler(BaseCrawler):
    """GeekNews Article 크롤러"""

    platform_name: str = "geeknews"
    URL_PATTERN: str = r"https?://(www\.)?news\.hada\.io/topic\?id=\d+"
    NOISE_SELECTORS: list[str]  # script, style, nav 등 노이즈 요소

    def __init__(
        self,
        include_comments: bool = False,  # 댓글 포함 여부 (기본: False)
        timeout: Optional[float] = None,
        headers: Optional[dict] = None,
    )

    # --- 추상 메서드 구현 ---
    def validate_url(self, url: str) -> bool
    async def extract(self, url: str) -> CrawledArticle | None
    def _parse_content(self, soup: BeautifulSoup, url: str) -> CrawledArticle | None

    # --- Private Helper 메서드 ---
    def _extract_title(self, soup: BeautifulSoup) -> str
    def _extract_original_url(self, soup: BeautifulSoup) -> str
    def _extract_meta_info(self, soup: BeautifulSoup) -> dict
    def _extract_main_content(self, soup: BeautifulSoup) -> str
    def _format_content(self, element) -> str  # 마크다운 포맷팅
    def _extract_comments(self, soup: BeautifulSoup) -> list[dict]
    def _get_comment_count(self, soup: BeautifulSoup) -> int
    def _extract_topic_id(self, url: str) -> Optional[str]
    def _build_content(...) -> str  # 데이터 조합
```

**콘텐츠 포맷팅 규칙:**

- `<ul>/<li>` → `• bullet point`
- `<h1>-<h6>` → `# ~ ###### 헤더`
- `<blockquote>` → `> 인용구`
- `<code>` → `` `인라인 코드` ``

**사용 예시:**

```python
from app.services.crawlers import GeekNewsCrawler

crawler = GeekNewsCrawler(include_comments=True)
article = await crawler.extract("https://news.hada.io/topic?id=24268")

print(article.title)              # 제목
print(article.content)            # 본문 + 메타정보 + 댓글
print(article.metadata.points)    # 포인트
print(article.metadata.author)    # 작성자
print(article.secondary_urls)     # [원본 외부 링크]
```

---

### 2-4. Medium 크롤러 (`medium.py`)

- 파일 위치: `backend/app/services/crawlers/medium.py`
- 참고: `reference/data_crawling/src/medium_crawler.py`

**Freedium 미러 사이트 활용:**

- Freedium: `https://freedium.cfd/{medium_url}`
- 봇 탐지 없음, 페이월 우회, 안정적인 HTML 구조
- 실패 시 원본 Medium URL로 자동 fallback

**URL 패턴:**

```
https://medium.com/@username/article-title-xxxxx
https://medium.com/publication/article-title-xxxxx
https://subdomain.medium.com/article-title-xxxxx
```

**추출 항목 (Freedium 기준):**

| 항목   | 선택자/방법                                  | 비고                           |
| ------ | -------------------------------------------- | ------------------------------ |
| 제목   | `h1` 또는 `<title>`                          | " - Freedium" 접미사 자동 제거 |
| 부제목 | `h2` 또는 `.subtitle`                        | 200자 미만인 경우에만 사용     |
| 작성자 | `.author`, `[rel="author"]`, `a[href*="/@"]` |                                |
| 게시일 | `<time>` 태그                                | datetime 속성 또는 텍스트      |
| 본문   | `main`, `article`, `.content`, `body`        | 우선순위 순 탐색               |

**추출 항목 (Medium 원본 - Fallback):**

| 항목      | 선택자/방법                                                             | 비고                         |
| --------- | ----------------------------------------------------------------------- | ---------------------------- |
| 제목      | `[data-testid="storyTitle"]` 또는 `h1`                                  | fallback: `<title>` 태그     |
| 부제목    | `.pw-subtitle-paragraph`                                                |                              |
| 작성자    | `[data-testid="authorName"]`                                            |                              |
| 게시일    | `[data-testid="storyPublishDate"]`                                      |                              |
| 읽는 시간 | `[data-testid="storyReadTime"]`                                         |                              |
| 박수 수   | `[data-testid="headerClapButton"]` / `footerClapButton`                 | 숫자만 추출 (K 단위 지원)    |
| 태그      | JSON-LD `keywords`                                                      | 리스트 또는 콤마 구분 문자열 |
| 본문      | `section` 또는 `article` 내 `h1-h6, p, blockquote, pre, ul, ol, figure` |                              |

**본문 포맷팅 규칙:**

- `<pre>` → \`\`\`코드블록\`\`\`
- `<blockquote>` → `> 인용구`
- `<ul>/<ol>` → `- item` 또는 `1. item`
- `<h1>-<h6>` → `# ~ ###### 헤더`
- `<figure>/<img>` → `[Image: alt](src)` + 캡션

**구현된 클래스:**

```python
class MediumCrawler(BaseCrawler):
    """Medium Article 크롤러 (Freedium 기반)"""

    platform_name: str = "medium"
    FREEDIUM_BASE_URL: str = "https://freedium.cfd"
    URL_PATTERNS: list[str]  # 다중 URL 패턴 지원
    DEFAULT_REQUEST_DELAY: float = 0.5  # Rate limiting 방지

    def __init__(
        self,
        timeout: Optional[float] = None,
        headers: Optional[dict] = None,
        request_delay: Optional[float] = None,  # 요청 지연 (초)
        use_freedium: bool = True,  # Freedium 사용 여부
    )

    # --- 추상 메서드 구현 ---
    def validate_url(self, url: str) -> bool
    async def extract(self, url: str) -> CrawledArticle | None
    def _parse_content(self, soup: BeautifulSoup, url: str) -> CrawledArticle | None

    # --- URL 변환 메서드 ---
    def _convert_to_freedium_url(self, url: str) -> str
    def _extract_original_url(self, freedium_url: str) -> str

    # --- Freedium 파싱 메서드 ---
    def _parse_freedium_content(self, soup: BeautifulSoup, url: str) -> CrawledArticle | None
    def _extract_freedium_title(self, soup: BeautifulSoup) -> str | None
    def _extract_freedium_metadata(self, soup: BeautifulSoup) -> dict
    def _extract_freedium_body(self, soup: BeautifulSoup) -> str

    # --- Medium 원본 파싱 메서드 (Fallback) ---
    def _parse_medium_content(self, soup: BeautifulSoup, url: str) -> CrawledArticle | None
    def _extract_medium_metadata(self, soup: BeautifulSoup) -> dict
    def _extract_medium_body(self, soup: BeautifulSoup) -> str

    # --- 공통 유틸리티 ---
    def _format_tag(self, tag) -> str | None  # 태그 → 마크다운 변환
    def _build_content(self, meta_info: dict, article_body: str) -> str
```

**사용 예시:**

```python
from app.services.crawlers import MediumCrawler

# 기본 사용 (Freedium 활성화)
crawler = MediumCrawler()
article = await crawler.extract("https://medium.com/@user/article-title-xxxxx")

# Freedium 비활성화 (원본 Medium 직접 크롤링)
crawler = MediumCrawler(use_freedium=False)
article = await crawler.extract("https://medium.com/@user/article-title-xxxxx")

# 요청 지연 커스텀
crawler = MediumCrawler(request_delay=1.0)  # 1초 대기

print(article.title)              # 제목
print(article.content)            # 본문 + 메타정보
print(article.metadata.author)    # 작성자
print(article.metadata.read_time) # 읽는 시간
print(article.metadata.subtitle)  # 부제목
```

**특이사항:**

- **Freedium 우선**: 기본적으로 Freedium 미러를 통해 콘텐츠 가져옴 (페이월 우회)
- **자동 Fallback**: Freedium 실패 시 원본 Medium URL로 자동 전환
- **Rate Limiting 방지**: `request_delay` 옵션으로 요청 전 대기 (기본 0.5초)
- **노이즈 제거 (2단계)**:
  - 셀렉터 기반: `script`, `style`, `nav`, `footer`, `button`, `.speechify-ignore`, `.grecaptcha-badge` 등
  - 텍스트 기반: `FREEDIUM_NOISE_TEXTS` 패턴 매칭으로 Freedium 자체 안내 섹션 제거 ("Reporting a Problem" 등)
- **MediumTextExtractor**: Medium/Freedium 특화 텍스트 추출기 클래스
- **중복 제거**: 본문 추출 시 `seen_texts` 집합으로 중복 텍스트 필터링

---

### 2-5. 크롤러 팩토리 (`factory.py`)

- 파일 위치: `backend/app/services/crawlers/factory.py`

**구현된 클래스:**

| 클래스                | 역할                                 |
| --------------------- | ------------------------------------ |
| `CrawlerFactory`      | URL 도메인 기반 크롤러 선택 팩토리   |
| `UnsupportedURLError` | 지원하지 않는 URL에 대한 커스텀 예외 |

**`CrawlerFactory` 메서드:**

| 메서드                           | 설명                                               |
| -------------------------------- | -------------------------------------------------- |
| `get_crawler(url, **kwargs)`     | URL에서 도메인 추출 후 적절한 크롤러 인스턴스 반환 |
| `get_supported_domains()`        | 지원하는 도메인 패턴 목록 반환                     |
| `is_supported(url)`              | URL이 지원되는지 확인 (bool 반환)                  |
| `register_crawler(domain, cls)`  | 런타임에 새 크롤러 동적 등록                       |
| `_match_domain(domain, pattern)` | 도메인 매칭 (서브도메인 지원)                      |

**특징:**

- 서브도메인 지원: `towardsdatascience.medium.com` → `MediumCrawler`
- `www.` 접두사 자동 제거
- 크롤러 생성자에 kwargs 전달 지원 (예: `include_comments=True`)
- `UnsupportedURLError` 커스텀 예외로 상세 정보 제공

**구현된 클래스 시그니처:**

```python
class UnsupportedURLError(ValueError):
    """지원하지 않는 URL에 대한 예외"""
    def __init__(self, url: str, domain: str)


class CrawlerFactory:
    """URL 도메인 기반 크롤러 선택 팩토리"""

    # 도메인 → 크롤러 클래스 매핑
    _crawlers: dict[str, type[BaseCrawler]] = {
        "news.hada.io": GeekNewsCrawler,
        "medium.com": MediumCrawler,
    }

    @classmethod
    def get_crawler(cls, url: str, **kwargs) -> BaseCrawler:
        """URL에서 도메인 추출 후 적절한 크롤러 반환"""

    @classmethod
    def get_supported_domains(cls) -> list[str]:
        """지원하는 도메인 목록 반환"""

    @classmethod
    def is_supported(cls, url: str) -> bool:
        """URL 지원 여부 확인"""

    @classmethod
    def register_crawler(cls, domain_pattern: str, crawler_cls: type[BaseCrawler]) -> None:
        """새로운 크롤러 동적 등록"""
```

**사용 예시:**

```python
from app.services.crawlers import CrawlerFactory, UnsupportedURLError

# 크롤러 가져오기
crawler = CrawlerFactory.get_crawler(
    "https://news.hada.io/topic?id=24268",
    include_comments=True
)
article = await crawler.extract(url)

# 지원 여부 확인
if CrawlerFactory.is_supported(url):
    crawler = CrawlerFactory.get_crawler(url)

# 지원 도메인 목록
domains = CrawlerFactory.get_supported_domains()
# ['news.hada.io', 'medium.com']

# 에러 처리
try:
    crawler = CrawlerFactory.get_crawler("https://example.com")
except UnsupportedURLError as e:
    print(f"도메인 {e.domain}은(는) 지원되지 않습니다")
```

**확장성:** 새 플랫폼 추가 시 `_crawlers` 딕셔너리에 한 줄만 추가, 또는 `register_crawler()` 메서드로 런타임 등록

---

### 2-6. 크롤링 API 엔드포인트 (`crawl.py`)

- 파일 위치: `backend/app/api/v1/crawl.py`

**구현된 엔드포인트:**

| 메서드 | 경로                              | 설명                             |
| ------ | --------------------------------- | -------------------------------- |
| POST   | `/api/v1/crawl`                   | URL 크롤링 후 정제된 아티클 반환 |
| GET    | `/api/v1/crawl/supported-domains` | 지원하는 도메인 목록 반환        |
| GET    | `/api/v1/crawl/check-support`     | URL 지원 여부 확인               |

**에러 코드:**

| 상태 코드 | 설명                               |
| --------- | ---------------------------------- |
| 400       | 지원하지 않는 URL 또는 잘못된 형식 |
| 500       | 크롤링 실패 (네트워크/파싱 오류)   |

**구현된 핸들러 시그니처:**

```python
router = APIRouter(prefix="/crawl", tags=["crawl"])

@router.post("", response_model=CleanedArticle, summary="URL 크롤링")
async def crawl_url(request: CrawlRequest) -> CleanedArticle:
    """
    URL을 받아 크롤링 후 정제된 아티클을 반환합니다.

    처리 흐름:
    1. CrawlerFactory로 적절한 크롤러 선택
    2. crawler.extract()로 콘텐츠 추출
    3. CleanedArticle.from_crawled()로 변환 후 반환
    """

@router.get("/supported-domains", response_model=list[str], summary="지원 도메인 목록")
async def get_supported_domains() -> list[str]:
    """지원하는 도메인 목록을 반환합니다."""

@router.get("/check-support", response_model=dict, summary="URL 지원 여부 확인")
async def check_url_support(url: str) -> dict:
    """URL이 지원되는지 확인합니다."""
```

**사용 예시:**

```bash
# URL 크롤링
curl -X POST "http://localhost:8000/api/v1/crawl" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://news.hada.io/topic?id=24268"}'

# 지원 도메인 확인
curl "http://localhost:8000/api/v1/crawl/supported-domains"
# ["news.hada.io", "medium.com"]

# URL 지원 여부 확인
curl "http://localhost:8000/api/v1/crawl/check-support?url=https://news.hada.io/topic?id=24268"
# {"url": "...", "is_supported": true, "platform": "geeknews"}
```

**main.py 라우터 등록 완료:**

```python
from app.api.v1 import crawl
app.include_router(crawl.router, prefix=settings.API_V1_STR)
```

---

### 2-7. 통합 테스트

- 실제 URL로 크롤링 동작 검증

---

#### 테스트 수행 방법

**방법 A: API 서버를 통한 테스트 (권장)**

```bash
# 1. 백엔드 서버 실행 (터미널 1)
cd backend
uv run uvicorn app.main:app --reload

# 2. 크롤링 테스트 (터미널 2)
# GeekNews 테스트
curl -X POST "http://localhost:8000/api/v1/crawl" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://news.hada.io/topic?id=24400"}'

# Medium 테스트
curl -X POST "http://localhost:8000/api/v1/crawl" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://medium.com/data-science-collective/how-to-use-deepseek-ocr-and-docling-for-pdf-parsing-ae00a88e10e7"}'

# 지원 도메인 확인
curl "http://localhost:8000/api/v1/crawl/supported-domains"

# URL 지원 여부 확인
curl "http://localhost:8000/api/v1/crawl/check-support?url=https://news.hada.io/topic?id=24400"

# 3. 저장된 파일 확인
ls -la backend/data/crawled/geeknews/
ls -la backend/data/crawled/medium/
```

**방법 B: Python 스크립트 직접 실행**

```python
# test_crawl.py (backend 폴더에 생성)
import asyncio
from app.services.crawlers import CrawlerFactory

async def test():
    # GeekNews 테스트
    url = "https://news.hada.io/topic?id=24400"
    crawler = CrawlerFactory.get_crawler(url)
    result = await crawler.extract(url)

    print(f"제목: {result.title}")
    print(f"플랫폼: {result.platform}")
    print(f"콘텐츠 길이: {len(result.content)} chars")
    print(f"작성자: {result.metadata.author}")
    print(result.model_dump_json(indent=2))

asyncio.run(test())
```

```bash
# 실행
cd backend
uv run python test_crawl.py
```

**방법 C: FastAPI Swagger UI**

1. 서버 실행: `cd backend && uv run uvicorn app.main:app --reload`
2. 브라우저에서 `http://localhost:8000/docs` 접속
3. `POST /api/v1/crawl` 엔드포인트 선택 → "Try it out" 클릭
4. Request body에 URL 입력 후 "Execute"

---

**테스트 케이스 및 결과:**

| #   | 테스트 케이스           | URL / 입력                                       | 결과                          |
| --- | ----------------------- | ------------------------------------------------ | ----------------------------- |
| 1   | GeekNews Article 크롤링 | `https://news.hada.io/topic?id=24400`            | ✅ 성공                       |
| 2   | Medium Article 크롤링   | `https://medium.com/data-science-collective/...` | ✅ 성공                       |
| 3   | 지원하지 않는 URL 에러  | `https://example.com/article`                    | ✅ `UnsupportedURLError` 발생 |
| 4   | 잘못된 URL 형식 처리    | `not-a-url`, `""`, `ftp://...`                   | ✅ `is_supported: False`      |

**GeekNews 크롤링 결과:**

```
✅ 크롤링 성공!
  - 제목: 스펙 주도 개발(SDD): 워터폴의 귀환
  - 플랫폼: geeknews
  - 작성자: GN⁺
  - 포인트: 20
  - 댓글 수: 1
  - 원본 URL: https://marmelab.com/blog/2025/11/12/spec-driven-development-waterfall-strikes-back.html
  - 콘텐츠 길이: 3037 chars
```

**Medium 크롤링 결과:**

```
✅ 크롤링 성공!
  - 제목: How To Use DeepSeek-OCR And Docling For PDF Parsing
  - 플랫폼: medium
  - 부제목: Is it really that good?
  - 콘텐츠 길이: 1320 chars
```

**CrawlerFactory 테스트 결과:**

```
지원 도메인 목록: ['news.hada.io', 'medium.com']

URL 지원 여부:
  https://news.hada.io/topic?id=24400 → True
  https://medium.com/@user/article → True
  https://example.com/article → False

에러 처리:
  ✅ UnsupportedURLError 발생!
    - 도메인: example.com
    - 메시지: 지원하지 않는 URL입니다: example.com
```

**수정된 버그:**

- `schemas.py`: 순환 import 문제 해결 (지연 import 적용)
- `base.py`: httpx 인코딩 처리 수정 (`apparent_encoding` → 자동 처리)
- `pyproject.toml`: `loguru` 의존성 추가

