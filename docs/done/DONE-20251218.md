# 개발 완료 이력 - 2025-12-18

> Phase 2 시작: 크롤러 기반 구조 구현 - 추상 기본 크롤러(ABC), 텍스트 추출기, 패키지 구조 설정

---

## Phase 2: 크롤링 및 ETL 아키텍처 (시작)

### 크롤러 기반 구조 구현 (Phase 2 시작)

- **추상 기본 크롤러 구현** (`backend/app/services/crawlers/base.py`):
  - `BaseTextExtractor`: 텍스트 정제 유틸리티 클래스
    - `clean_text()`: 공백/줄바꿈 정규화 (3줄↑ → 2줄, 탭/연속공백 → 스페이스 1개)
    - `remove_noise_elements()`: 광고, 네비게이션 등 노이즈 요소 제거
    - `extract_text_from_element()`: BeautifulSoup 요소에서 텍스트 추출
  - `BaseCrawler` (ABC): 모든 크롤러의 추상 기본 클래스
    - 공통 메서드: `fetch_html()` (httpx 비동기), `parse_html()`, `extract_og_meta()`, `_build_metadata()`
    - 추상 메서드: `validate_url()`, `extract()`, `_parse_content()`
    - 2차 URL 크롤링 stub: `extract_secondary_urls()`, `crawl_secondary()`
- **패키지 초기화 파일 생성**:
  - `backend/app/services/__init__.py`: services 패키지 초기화
  - `backend/app/services/crawlers/__init__.py`: 공개 API export 설정

**생성된 파일:**

| 파일                                         | 내용                                               |
| -------------------------------------------- | -------------------------------------------------- |
| `backend/app/services/__init__.py`           | services 패키지 초기화                             |
| `backend/app/services/crawlers/__init__.py`  | 공개 API export 설정                               |
| `backend/app/services/crawlers/base.py`      | `BaseCrawler` ABC + `BaseTextExtractor` 유틸리티   |
| `backend/app/services/crawlers/schemas.py`   | Pydantic 데이터 모델 정의                          |

**디렉토리 구조:**

```
backend/app/
├── services/
│   ├── __init__.py           # services 패키지 초기화
│   └── crawlers/
│       ├── __init__.py       # 공개 API export
│       ├── schemas.py        # Pydantic 데이터 모델
│       └── base.py           # BaseCrawler ABC + BaseTextExtractor
```

**`BaseTextExtractor` 클래스:**

```python
class BaseTextExtractor:
    """텍스트 정제 유틸리티 클래스"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """공백/줄바꿈 정규화"""
        # 3줄 이상의 연속 줄바꿈 → 2줄로
        # 탭/연속공백 → 스페이스 1개
    
    @staticmethod
    def remove_noise_elements(soup: BeautifulSoup, selectors: list[str]) -> None:
        """광고, 네비게이션 등 노이즈 요소 제거"""
    
    @staticmethod
    def extract_text_from_element(element) -> str:
        """BeautifulSoup 요소에서 텍스트 추출"""
```

**`BaseCrawler` ABC 핵심 구조:**

```python
class BaseCrawler(ABC):
    """모든 크롤러의 추상 기본 클래스"""
    
    platform_name: str = "base"
    DEFAULT_HEADERS: dict  # User-Agent, Accept 등
    
    # 공통 메서드 (구현됨)
    async def fetch_html(self, url: str) -> str | None
    def parse_html(self, html: str) -> BeautifulSoup
    def extract_og_meta(self, soup: BeautifulSoup) -> dict
    def _build_metadata(self, og_meta: dict, **extra_fields) -> ArticleMetadata
    
    # 추상 메서드 (하위 클래스에서 구현 필수)
    @abstractmethod
    def validate_url(self, url: str) -> bool
    
    @abstractmethod
    async def extract(self, url: str) -> CrawledArticle | None
    
    @abstractmethod
    def _parse_content(self, soup: BeautifulSoup, url: str) -> CrawledArticle | None
```

**Pydantic 스키마 (`schemas.py`):**

```python
class CrawlRequest(BaseModel):
    url: HttpUrl

class ArticleMetadata(BaseModel):
    og_title: str | None = None
    og_description: str | None = None
    og_image: str | None = None
    og_url: str | None = None
    author: str | None = None
    published_time: str | None = None
    # ... 플랫폼별 확장 필드

class CrawledArticle(BaseModel):
    title: str
    content: str
    url: str
    platform: str
    metadata: ArticleMetadata
    secondary_urls: list[str] = []
    crawled_at: datetime

class CleanedArticle(BaseModel):
    # CrawledArticle에서 API 응답용으로 변환
    title: str
    cleaned_content: str
    preview_text: str  # 300자 미리보기
    url: str
    platform: str
    metadata: ArticleMetadata
    crawled_at: datetime
```

**다음 단계 (예정):**

- [ ] `geeknews.py`: GeekNews Article 크롤러 구현
- [ ] `medium.py`: Medium Article 크롤러 구현
- [ ] `factory.py`: URL 기반 크롤러 팩토리 구현
- [ ] `crawl.py`: API 엔드포인트 구현

