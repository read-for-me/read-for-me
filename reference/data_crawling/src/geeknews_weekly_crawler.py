"""
GeekNews Weekly Crawler

GeekNews Weekly ë‰´ìŠ¤ë ˆí„° í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
URL í˜•ì‹: https://news.hada.io/weekly/YYYYWW (ì˜ˆ: /weekly/202546)

Usage:
    python geeknews_weekly_crawler.py https://news.hada.io/weekly/202546 --gcs
    python geeknews_weekly_crawler.py https://news.hada.io/weekly/202546 --gcs --no-local
"""

import argparse
import json
import re
from pathlib import Path
from typing import Optional, List
from datetime import datetime

from bs4 import BeautifulSoup
from loguru import logger

from base_crawler import CrawledContent
from geeknews_base import GeekNewsBaseCrawler


class GeekNewsWeeklyCrawler(GeekNewsBaseCrawler):
    """
    GeekNews Weekly í¬ë¡¤ëŸ¬
    
    Weekly ë‰´ìŠ¤ë ˆí„° í˜ì´ì§€ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤:
    - ì œëª© (ì£¼ì°¨ ì •ë³´ í¬í•¨)
    - ë‚ ì§œ ë²”ìœ„
    - ë©”ì¸ ì„¤ëª… (desc)
    - í† í”½ ë¦¬ìŠ¤íŠ¸ (topics) -> JSONLë¡œë„ ì €ì¥ (Local / GCS)
    """
    
    platform_name: str = "geeknews_weekly"
    URL_PATTERN: str = r"https?://(www\.)?news\.hada\.io/weekly/\d+"
    
    def __init__(
        self, 
        output_dir: str = "./geeknews/weekly", 
        timeout: int = 30,
        save_local: bool = True,
        save_gcs: bool = False
    ):
        # ìƒìœ„ í´ë˜ìŠ¤(GeekNewsBaseCrawler -> BaseCrawler)ì— ì €ì¥ ì„¤ì • ì „ë‹¬
        super().__init__(
            output_dir=output_dir, 
            timeout=timeout,
            save_local=save_local,
            save_gcs=save_gcs
        )
        logger.info(f"Initialized GeekNewsWeeklyCrawler (Local={save_local}, GCS={save_gcs})")
    
    def _parse_content(self, soup: BeautifulSoup, url: str) -> Optional[CrawledContent]:
        """
        Weekly í˜ì´ì§€ì—ì„œ ì½˜í…ì¸ ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
        """
        try:
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            if not title:
                logger.warning("Failed to extract title")
                title = "GeekNews Weekly"
            
            # ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ
            date_range = self._extract_date_range(soup)
            
            # ë©”ì¸ ì„¤ëª… ì¶”ì¶œ
            description = self._extract_description(soup)
            
            # í† í”½ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            topics = self._extract_topics(soup)
            
            # ì „ì²´ ì½˜í…ì¸  ì¡°í•© (í…ìŠ¤íŠ¸ íŒŒì¼ìš©)
            content_parts = []
            
            if date_range:
                content_parts.append(f"ğŸ“… {date_range}\n")
            
            if description:
                content_parts.append("ğŸ“ Description")
                content_parts.append("-" * 40)
                content_parts.append(description)
                content_parts.append("")
            
            if topics:
                content_parts.append("ğŸ“‹ Topics")
                content_parts.append("-" * 40)
                for i, topic in enumerate(topics, 1):
                    content_parts.append(f"\n[{i}] {topic.get('title', 'No Title')}")
                    if topic.get('url'):
                        content_parts.append(f"    ğŸ”— {topic['url']}")
                    if topic.get('description'):
                        content_parts.append(f"    {topic['description']}")
            
            content = "\n".join(content_parts)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self._extract_og_meta(soup)
            metadata["date_range"] = date_range
            metadata["topic_count"] = len(topics)
            metadata["raw_topics"] = topics
            
            # ì£¼ì°¨ ë²ˆí˜¸ ì¶”ì¶œ
            week_match = re.search(r'/weekly/(\d+)', url)
            if week_match:
                metadata["week_id"] = week_match.group(1)
            
            return CrawledContent(
                title=title,
                content=content,
                url=url,
                platform=self.platform_name,
                metadata=metadata
            )
            
        except Exception as e:
            logger.error(f"Error parsing weekly content: {e}")
            return None

    def crawl_and_save(self, url: str, filename: Optional[str] = None) -> Optional[Path]:
        """
        URLì„ í¬ë¡¤ë§í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤ (Text + JSONL).
        Local ë° GCS ì €ì¥ ì—¬ë¶€ëŠ” ì´ˆê¸°í™” ì‹œ ì„¤ì •ëœ ê°’ì„ ë”°ë¦…ë‹ˆë‹¤.
        """
        # 1. í¬ë¡¤ë§ ìˆ˜í–‰
        content = self.extract(url)
        if content is None:
            logger.error(f"Failed to extract content from: {url}")
            return None
        
        # 2. ë™ì  ë””ë ‰í† ë¦¬ ê²½ë¡œ ê³„ì‚° (YYYY_MM_WeekNN)
        date_range = content.metadata.get("date_range", "")
        week_id = content.metadata.get("week_id", "")
        
        folder_name = "unknown_week"
        date_match = re.search(r'(\d{4})-(\d{2})', date_range)
        if date_match and week_id:
            year, month = date_match.groups()
            week_num = week_id[-2:] 
            folder_name = f"{year}_{month}_Week{week_num}"
        elif week_id:
            folder_name = f"Week_{week_id}"
            
        # 3. ì„ì‹œë¡œ output_dir ë³€ê²½ (BaseCrawlerê°€ ì´ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥í•¨)
        original_output_dir = self.output_dir
        self.output_dir = self.output_dir / folder_name
        
        # ë¡œì»¬ ì €ì¥ì„ ìœ„í•´ ë””ë ‰í† ë¦¬ ìƒì„± (Local ì˜µì…˜ì´ ì¼œì ¸ìˆì„ ë•Œë§Œ)
        if self.save_local:
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # 4. ë©”ì¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ (BaseCrawlerì˜ ë¡œì§ ìœ„ì„: Local + GCS)
            # save_to_file ë‚´ë¶€ì—ì„œ self.save_local, self.save_gcs ì²´í¬í•¨
            saved_path = self.save_to_file(content, filename)
            
            # íŒŒì¼ëª… ê²°ì • (saved_pathê°€ ì—†ìœ¼ë©´-ë¡œì»¬ì €ì¥X- ì§ì ‘ ìƒì„±)
            if saved_path:
                base_filename = saved_path.stem
            else:
                if filename:
                    base_filename = Path(filename).stem
                else:
                    safe_title = self._sanitize_filename(content.title)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    base_filename = f"{content.platform}_{safe_title}_{timestamp}"

            # 5. JSONL íŒŒì¼ ì¶”ê°€ ì €ì¥ (Local + GCS)
            if content.metadata.get("raw_topics"):
                self._process_topics_jsonl(
                    topics=content.metadata["raw_topics"], 
                    base_filename=base_filename,
                    folder_name=folder_name  # GCS ê²½ë¡œ êµ¬ì„±ì„ ìœ„í•´ í•„ìš”
                )
                
            return saved_path
        finally:
            # ë””ë ‰í† ë¦¬ ë³µêµ¬
            self.output_dir = original_output_dir

    def _process_topics_jsonl(self, topics: List[dict], base_filename: str, folder_name: str):
        """
        í† í”½ ë¦¬ìŠ¤íŠ¸ë¥¼ JSONL í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥(Local) ë° ì—…ë¡œë“œ(GCS)í•©ë‹ˆë‹¤.
        BaseCrawlerëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ, JSONL ê°™ì€ íŒŒìƒ íŒŒì¼ì€ ì—¬ê¸°ì„œ ì§ì ‘ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        # 1. JSONL ì»¨í…ì¸  ìƒì„±
        jsonl_lines = []
        for topic in topics:
            entry = {
                "url": topic.get("url", ""),
                "headline": topic.get("title", ""),
                "type": "article"
            }
            if entry["url"] and entry["headline"]:
                jsonl_lines.append(json.dumps(entry, ensure_ascii=False))
        
        if not jsonl_lines:
            return

        jsonl_content = "\n".join(jsonl_lines)
        jsonl_filename = f"{base_filename}.jsonl"

        # 2. Local ì €ì¥
        if self.save_local:
            local_path = self.output_dir / jsonl_filename
            if local_path.exists():
                logger.info(f"â­ï¸  Skipped JSONL local save (Duplicate): {local_path}")
            else:
                try:
                    with open(local_path, "w", encoding="utf-8") as f:
                        f.write(jsonl_content)
                    logger.info(f"ğŸ’¾ Saved JSONL locally: {local_path}")
                except Exception as e:
                    logger.error(f"âŒ Failed to save JSONL locally: {e}")

        # 3. GCS ì—…ë¡œë“œ
        if self.save_gcs and self.gcs_handler:
            try:
                # GCS ê²½ë¡œ: geeknews_weekly/YYYY_MM_WeekNN/filename.jsonl
                # BaseCrawler êµ¬ì¡°ì— ë§ì¶”ê¸° ìœ„í•´ output_dir êµ¬ì¡° ë°˜ì˜
                # self.output_dir.nameì€ í˜„ì¬ ë™ì ìœ¼ë¡œ ë³€ê²½ëœ ìƒíƒœì„ (folder_name)
                
                # ê²½ë¡œ êµ¬ì„±: platform_name / dynamic_folder / filename
                # (BaseCrawler êµ¬í˜„ ë°©ì‹ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥, ì—¬ê¸°ì„œëŠ” ëª…ì‹œì  ê²½ë¡œ ì‚¬ìš©)
                gcs_path = f"{self.platform_name}/{folder_name}/{jsonl_filename}"

                if self.gcs_handler.file_exists(gcs_path):
                    logger.info(f"â­ï¸  Skipped JSONL GCS upload (Duplicate): gs://{self.gcs_handler.bucket_name}/{gcs_path}")
                else:
                    self.gcs_handler.upload_string(jsonl_content, gcs_path)
                
            except Exception as e:
                logger.error(f"âŒ Failed to upload JSONL to GCS: {e}")

    def _extract_title(self, soup: BeautifulSoup) -> str:
        title_elem = soup.select_one(".weekly-container h2")
        if title_elem:
            return self.text_extractor.clean_text(title_elem.get_text(strip=True))
        title_tag = soup.find("title")
        if title_tag:
            return self.text_extractor.clean_text(title_tag.get_text(strip=True))
        return ""
    
    def _extract_date_range(self, soup: BeautifulSoup) -> str:
        date_elem = soup.select_one(".date.center")
        if date_elem:
            return self.text_extractor.clean_text(date_elem.get_text(strip=True))
        return ""
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        desc_elem = soup.select_one(".desc")
        if desc_elem:
            for elem in desc_elem.select("hr, .date"):
                elem.decompose()
            return self.text_extractor.clean_text(desc_elem.get_text(separator="\n", strip=True))
        return ""
    
    def _extract_topics(self, soup: BeautifulSoup) -> list[dict]:
        topics = []
        topics_elem = soup.select_one(".topics")
        if topics_elem is None:
            return topics
        
        for li in topics_elem.select("li"):
            topic = {}
            link = li.select_one("a.link")
            if not link:
                link = li.select_one("a[href]")
            
            if link:
                topic["title"] = link.get_text(strip=True)
                href = link.get("href", "")
                if href:
                    if not href.startswith("http"):
                        topic["url"] = f"{self.BASE_URL}{href}"
                    else:
                        topic["url"] = href
            
            content_elem = li.select_one(".content")
            if content_elem:
                topic["description"] = self.text_extractor.clean_text(content_elem.get_text(strip=True))
            else:
                p_elem = li.select_one("p")
                if p_elem:
                    topic["description"] = self.text_extractor.clean_text(p_elem.get_text(strip=True))
            
            if topic.get("title") and topic.get("url"):
                topics.append(topic)
        return topics


def main():
    """CLI ì§„ì…ì """
    parser = argparse.ArgumentParser(
        description="GeekNews Weekly í¬ë¡¤ëŸ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument("url", help="í¬ë¡¤ë§í•  GeekNews Weekly URL")
    parser.add_argument("--output", "-o", default="geeknews/weekly", help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./geeknews/weekly)")
    parser.add_argument("--filename", "-f", default=None, help="ì €ì¥í•  íŒŒì¼ëª…")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    
    # GCS ë° ì €ì¥ ì˜µì…˜
    parser.add_argument("--gcs", action="store_true", help="Google Cloud Storage ì—…ë¡œë“œ í™œì„±í™”")
    parser.add_argument("--no-local", action="store_true", help="ë¡œì»¬ ì €ì¥ ë¹„í™œì„±í™”")
    
    args = parser.parse_args()
    
    if not args.verbose:
        logger.remove()
        logger.add(lambda msg: print(msg), level="INFO")
    
    save_local = not args.no_local
    
    with GeekNewsWeeklyCrawler(
        output_dir=args.output,
        save_local=save_local,
        save_gcs=args.gcs
    ) as crawler:
        result = crawler.crawl_and_save(args.url, filename=args.filename)
        
        if result or args.gcs: # ë¡œì»¬ ì €ì¥ì´ ì—†ì–´ë„ GCSê°€ ì„±ê³µí•˜ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            print(f"\nâœ… Crawling completed for: {args.url}")
        else:
            print(f"\nâŒ Failed to crawl: {args.url}")
            exit(1)


if __name__ == "__main__":
    main()