"""
Medium Crawler Module

Medium ì•„í‹°í´ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
BaseCrawlerë¥¼ ìƒì†ë°›ì•„ OOP êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©°, Mediumì˜ ë³µì¡í•œ DOM êµ¬ì¡°ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.

Usage:
    python medium_crawler.py https://medium.com/@shahbhat/building-a-production-grade-enterprise-ai-platform-with-vllm-a-complete-guide-from-the-trenches-cf8e7a7bdfb6
    python medium_crawler.py https://medium.com/... --output ./medium_docs
"""

import argparse
import re
import json
from typing import Optional, List, Dict

from bs4 import BeautifulSoup, Tag
from loguru import logger

from base_crawler import BaseCrawler, CrawledContent, BaseTextExtractor


class MediumTextExtractor(BaseTextExtractor):
    """
    Medium í˜ì´ì§€ íŠ¹í™” í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°
    Mediumì˜ ë¶ˆí•„ìš”í•œ UI ìš”ì†Œ(ë¡œê·¸ì¸ ë²„íŠ¼, ì•± ì—´ê¸° ë°°ë„ˆ ë“±)ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    REMOVE_SELECTORS = [
        "script", "style", "noscript", "iframe",
        "nav", "footer", 
        "button", 
        "[data-testid='headerSignUpButton']",
        "[data-testid='headerSignInButton']",
        ".speechify-ignore",  # ì˜¤ë””ì˜¤ ë“£ê¸° ë²„íŠ¼ ê´€ë ¨ í…ìŠ¤íŠ¸
        ".grecaptcha-badge"
    ]

    def clean_medium_html(self, soup: BeautifulSoup) -> BeautifulSoup:
        """HTMLì—ì„œ ë…¸ì´ì¦ˆ ìš”ì†Œë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        for selector in self.REMOVE_SELECTORS:
            for element in soup.select(selector):
                element.decompose()
        return soup


class MediumCrawler(BaseCrawler):
    """
    Medium ì•„í‹°í´ í¬ë¡¤ëŸ¬
    
    íŠ¹ì§•:
    - data-testid ì†ì„±ì„ í™œìš©í•œ ì•ˆì •ì ì¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    - ë³¸ë¬¸ ë‚´ ì½”ë“œ ë¸”ë¡(pre), ì¸ìš©êµ¬(blockquote), ë¦¬ìŠ¤íŠ¸ ë³´ì¡´
    - JSON-LD ë°ì´í„°ë¥¼ í†µí•œ ë³´ì¡° ì •ë³´ ì¶”ì¶œ
    """
    
    platform_name: str = "medium"
    # Medium í‘œì¤€ URL ë° ì»¤ìŠ¤í…€ ë„ë©”ì¸ ëŒ€ì‘ì„ ìœ„í•œ íŒ¨í„´ (ëŠìŠ¨í•œ ê²€ì‚¬)
    URL_PATTERN: str = r"https?://.*medium\.com/.*|https?://.*" 
    
    def __init__(
        self, 
        output_dir: str = "./medium_articles", 
        timeout: int = 30,
        save_local: bool = True,
        save_gcs: bool = False
    ):
        # BaseCrawlerë¡œ ì˜µì…˜ ì „ë‹¬
        super().__init__(
            output_dir=output_dir, 
            timeout=timeout,
            save_local=save_local,
            save_gcs=save_gcs
        )
        self.text_extractor = MediumTextExtractor()
        logger.info(f"Initialized MediumCrawler (Local={save_local}, GCS={save_gcs})")

    def extract(self, url: str) -> Optional[CrawledContent]:
        """URL ê²€ì¦ ë° ì½˜í…ì¸  ì¶”ì¶œ ì‹¤í–‰"""
        # HTML ê°€ì ¸ì˜¤ê¸°
        html = self.fetch_html(url)
        if html is None:
            return None
        
        soup = self.parse_html(html)
        
        # Medium í˜ì´ì§€ ì—¬ë¶€ í™•ì¸ (meta íƒœê·¸ ë“±ìœ¼ë¡œ 2ì°¨ ê²€ì¦ ê°€ëŠ¥)
        if not soup.select_one("meta[property='al:ios:app_name'][content='Medium']"):
            logger.warning(f"URL might not be a Medium article: {url}")

        return self._parse_content(soup, url)

    def _parse_content(self, soup: BeautifulSoup, url: str) -> Optional[CrawledContent]:
        """Medium HTML íŒŒì‹± ë¡œì§"""
        try:
            # 1. ë…¸ì´ì¦ˆ ì œê±°
            soup = self.text_extractor.clean_medium_html(soup)

            # 2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì œëª©, ì‘ì„±ì, ë‚ ì§œ ë“±)
            meta_info = self._extract_metadata(soup)
            
            # 3. ë³¸ë¬¸ ì¶”ì¶œ
            content_body = self._extract_article_body(soup)
            
            # 4. ìµœì¢… í…ìŠ¤íŠ¸ ì¡°ë¦½
            full_content = []
            
            # í—¤ë” ì •ë³´
            if meta_info.get("subtitle"):
                full_content.append(f"ğŸ“ Subtitle: {meta_info['subtitle']}\n")
            
            info_line = []
            if meta_info.get("author"): info_line.append(f"Author: {meta_info['author']}")
            if meta_info.get("date"): info_line.append(f"Date: {meta_info['date']}")
            if meta_info.get("read_time"): info_line.append(f"Read Time: {meta_info['read_time']}")
            if meta_info.get("claps"): info_line.append(f"ğŸ‘ Claps: {meta_info['claps']}")
            
            if info_line:
                full_content.append(" | ".join(info_line))
                full_content.append("-" * 40 + "\n")
            
            # ë³¸ë¬¸ ë‚´ìš©
            full_content.append(content_body)
            
            # íƒœê·¸ ì •ë³´
            if meta_info.get("tags"):
                full_content.append("\n" + "-" * 40)
                full_content.append(f"ğŸ·ï¸ Tags: {', '.join(meta_info['tags'])}")

            return CrawledContent(
                title=meta_info.get("title", "Untitled Medium Article"),
                content="\n".join(full_content),
                url=url,
                platform=self.platform_name,
                metadata=meta_info
            )

        except Exception as e:
            logger.error(f"Error parsing Medium content: {e}")
            return None

    def _extract_metadata(self, soup: BeautifulSoup) -> Dict:
        """
        data-testid ë° OpenGraph íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        """
        meta = {}
        
        # ì œëª©
        title_elem = soup.select_one('[data-testid="storyTitle"]') or soup.select_one('h1.pw-post-title')
        if title_elem:
            meta["title"] = self.text_extractor.clean_text(title_elem.get_text())
        else:
            meta["title"] = soup.find("title").get_text() if soup.find("title") else ""

        # ë¶€ì œëª© (h2 ë“±) - ë³´í†µ ì œëª© ë°”ë¡œ ë’¤ì— ìœ„ì¹˜
        subtitle_elem = soup.select_one('.pw-subtitle-paragraph')
        if subtitle_elem:
            meta["subtitle"] = self.text_extractor.clean_text(subtitle_elem.get_text())

        # ì‘ì„±ì
        author_elem = soup.select_one('[data-testid="authorName"]')
        if author_elem:
            meta["author"] = self.text_extractor.clean_text(author_elem.get_text())

        # ê²Œì‹œì¼
        date_elem = soup.select_one('[data-testid="storyPublishDate"]')
        if date_elem:
            meta["date"] = self.text_extractor.clean_text(date_elem.get_text())

        # ì½ëŠ” ì‹œê°„
        read_time_elem = soup.select_one('[data-testid="storyReadTime"]')
        if read_time_elem:
            meta["read_time"] = self.text_extractor.clean_text(read_time_elem.get_text())

        # ë°•ìˆ˜(Claps) ìˆ˜
        # data-testid="headerClapButton" ë‚´ë¶€ í˜¹ì€ footerClapButton ë‚´ë¶€ì˜ ìˆ«ì í™•ì¸
        clap_elem = soup.select_one('[data-testid="headerClapButton"]') or soup.select_one('[data-testid="footerClapButton"]')
        if clap_elem:
            # ë²„íŠ¼ ì•ˆì˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ ì‹œë„
            clap_text = clap_elem.get_text(strip=True)
            # "96" ê°™ì€ ìˆ«ìë§Œ ë‚¨ê¸°ê¸° (ì•„ì´ì½˜ í…ìŠ¤íŠ¸ ì œê±°)
            meta["claps"] = re.sub(r'[^0-9K\.]', '', clap_text)

        # JSON-LDì—ì„œ íƒœê·¸ë‚˜ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ì„ íƒì )
        script_json = soup.find("script", type="application/ld+json")
        if script_json:
            try:
                data = json.loads(script_json.string)
                if isinstance(data, dict):
                    # í‚¤ì›Œë“œ/íƒœê·¸ ì¶”ì¶œ ì‹œë„ (schema.org í‘œì¤€ì— ë”°ë¦„)
                    if "keywords" in data:
                        meta["tags"] = data["keywords"] if isinstance(data["keywords"], list) else data["keywords"].split(",")
            except json.JSONDecodeError:
                pass

        return meta

    def _extract_article_body(self, soup: BeautifulSoup) -> str:
        """
        Medium ë³¸ë¬¸ êµ¬ì¡°ë¥¼ ìˆœíšŒí•˜ë©° í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í¬ë§·íŒ…
        """
        # Mediumì˜ ë³¸ë¬¸ì€ ë³´í†µ section íƒœê·¸ ì•ˆì— ìˆìŒ
        article_content = soup.select_one('section')
        if not article_content:
            # fallback: article íƒœê·¸ ì „ì²´ ì‚¬ìš©
            article_content = soup.select_one('article')
            
        if not article_content:
            return ""

        content_parts = []
        
        # ë³¸ë¬¸ ë‚´ì˜ ëª¨ë“  ì˜ë¯¸ ìˆëŠ” íƒœê·¸ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        # Medium uses: h1-h6, p, blockquote, pre, ul, ol, figure
        tags = article_content.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'blockquote', 'pre', 'ul', 'ol', 'figure'])
        
        for tag in tags:
            # ì´ë¯¸ì§€ ìº¡ì…˜ ì²˜ë¦¬ (figure)
            if tag.name == 'figure':
                img = tag.find('img')
                caption = tag.find('figcaption')
                if img:
                    alt_text = img.get('alt', '')
                    src = img.get('src', '')
                    caption_text = caption.get_text(strip=True) if caption else ""
                    
                    # ì´ë¯¸ì§€ëŠ” ë§í¬ í˜•íƒœë¡œ í‘œì‹œ
                    content_parts.append(f"\n[Image: {alt_text}]({src})")
                    if caption_text:
                        content_parts.append(f"  â””â”€ <caption>: {caption_text}")
                continue

            # ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬ (pre)
            if tag.name == 'pre':
                code_text = tag.get_text(separator="\n", strip=True)
                content_parts.append(f"\n```\n{code_text}\n```\n")
                continue

            # ì¸ìš©êµ¬ ì²˜ë¦¬
            if tag.name == 'blockquote':
                quote_text = self.text_extractor.clean_text(tag.get_text())
                content_parts.append(f"\n> {quote_text}\n")
                continue

            # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
            if tag.name in ['ul', 'ol']:
                for li in tag.find_all('li'):
                    marker = "-" if tag.name == 'ul' else "1."
                    content_parts.append(f"{marker} {self.text_extractor.clean_text(li.get_text())}")
                content_parts.append("") # ë¦¬ìŠ¤íŠ¸ ëì— ì¤„ë°”ê¿ˆ
                continue

            # í—¤ë” ì²˜ë¦¬
            if tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(tag.name[1])
                text = self.text_extractor.clean_text(tag.get_text())
                content_parts.append(f"\n{'#' * level} {text}\n")
                continue

            # ì¼ë°˜ ë¬¸ë‹¨ (p)
            text = self.text_extractor.clean_text(tag.get_text())
            if text:
                content_parts.append(text)

        return "\n".join(content_parts)


def main():
    parser = argparse.ArgumentParser(description="Medium Article Crawler")
    parser.add_argument("url", help="Target Medium Article URL")
    parser.add_argument("--output", "-o", default="medium_articles", help="Output directory")

    # GCS Flags
    parser.add_argument("--gcs", action="store_true", help="Upload to GCS")
    parser.add_argument("--no-local", action="store_true", help="Do not save locally")
    
    args = parser.parse_args()

    logger.remove()
    logger.add(lambda msg: print(msg), level="INFO")
    
    save_local = not args.no_local
    
    # ë¡œê¹… ì„¤ì • (ê°„ë‹¨íˆ)
    logger.remove()
    logger.add(lambda msg: print(msg), level="INFO")

    # BaseCrawlerë¥¼ ìƒì†ë°›ì•˜ìœ¼ë¯€ë¡œ initì— ì¸ì ì „ë‹¬ ê°€ëŠ¥
    with MediumCrawler(
        output_dir=args.output, 
        save_local=save_local, 
        save_gcs=args.gcs
    ) as crawler:
        filepath = crawler.crawl_and_save(args.url)
        if filepath:
            print(f"\nâœ… Saved successfully to: {filepath}")
        else:
            print(f"\nâŒ Failed to crawl.")
            
if __name__ == "__main__":
    main()