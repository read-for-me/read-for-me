"""
GeekNews Article Crawler

GeekNews ê°œë³„ ì•„í‹°í´(í† í”½) í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
URL í˜•ì‹: https://news.hada.io/topic?id=XXXXX (ì˜ˆ: /topic?id=24268)

Usage:
    python geeknews_article_crawler.py https://news.hada.io/topic?id=24268
    python geeknews_article_crawler.py https://news.hada.io/topic?id=24268 --output ./my_output
"""

import re
import json
import argparse
from pathlib import Path
from typing import Optional

from bs4 import BeautifulSoup
from loguru import logger

from base_crawler import CrawledContent
from geeknews_base import GeekNewsBaseCrawler


class GeekNewsArticleCrawler(GeekNewsBaseCrawler):
    """
    GeekNews Article í¬ë¡¤ëŸ¬
    
    ê°œë³„ ì•„í‹°í´(í† í”½) í˜ì´ì§€ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤:
    - ì œëª©
    - ì›ë³¸ URL (ì™¸ë¶€ ë§í¬)
    - ì‘ì„±ì ì •ë³´
    - í¬ì¸íŠ¸ ë° ëŒ“ê¸€ ìˆ˜
    - ë³¸ë¬¸ ë‚´ìš© (topic_contents)
    - ëŒ“ê¸€ (comment_contents) ë° ëŒ€ëŒ“ê¸€ êµ¬ì¡° í¬í•¨
    """
    
    platform_name: str = "geeknews_article"
    URL_PATTERN: str = r"https?://(www\.)?news\.hada\.io/topic\?id=\d+"
    
    def __init__(
        self,
        output_dir: str = "./geeknews/articles",
        timeout: int = 30,
        include_comments: bool = True,
        save_local: bool = True,
        save_gcs: bool = False
    ):
        # ìƒìœ„ í´ë˜ìŠ¤ ì´ˆê¸°í™” (BaseCrawlerê°€ ì €ì¥ ì˜µì…˜ì„ ì²˜ë¦¬í•¨)
        super().__init__(
            output_dir=output_dir, 
            timeout=timeout,
            save_local=save_local,
            save_gcs=save_gcs
        )
        self.include_comments = include_comments
        logger.info(f"Initialized GeekNewsArticleCrawler (Comments={include_comments}, Local={save_local}, GCS={save_gcs})")
    
    def _parse_content(self, soup: BeautifulSoup, url: str) -> Optional[CrawledContent]:
        """
        Article í˜ì´ì§€ì—ì„œ ì½˜í…ì¸ ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        try:
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            if not title:
                logger.warning("Failed to extract title")
                title = "GeekNews Article"
            
            # ì›ë³¸ ë§í¬ ì¶”ì¶œ
            original_url = self._extract_original_url(soup)
            
            # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ (ì‘ì„±ì, í¬ì¸íŠ¸, ì‹œê°„)
            meta_info = self._extract_meta_info(soup)
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            main_content = self._extract_main_content(soup)
            
            # ëŒ“ê¸€ ì¶”ì¶œ (ì˜µì…˜)
            comments = []
            if self.include_comments:
                comments = self._extract_comments(soup)
            
            # ì „ì²´ ì½˜í…ì¸  ì¡°í•©
            content_parts = []
            
            # ì›ë³¸ ë§í¬
            if original_url:
                content_parts.append(f"ğŸ”— Original: {original_url}")
                content_parts.append("")
            
            # ë©”íƒ€ ì •ë³´
            if meta_info:
                meta_str = " | ".join([f"{k}: {v}" for k, v in meta_info.items() if v])
                content_parts.append(f"ğŸ“Š {meta_str}")
                content_parts.append("")
            
            # ë³¸ë¬¸
            if main_content:
                content_parts.append("ğŸ“ Content")
                content_parts.append("-" * 40)
                content_parts.append(main_content)
            
            # ëŒ“ê¸€ [ë³´ì™„ëœ ì¶œë ¥ ë¡œì§]
            if comments:
                content_parts.append("")
                content_parts.append(f"ğŸ’¬ Comments ({len(comments)})")
                content_parts.append("-" * 40)
                for i, comment in enumerate(comments, 1):
                    # Depthì— ë”°ë¥¸ ë“¤ì—¬ì“°ê¸° ì²˜ë¦¬
                    depth = comment.get('depth', 0)
                    indent = "    " * depth
                    marker = "â””â”€ " if depth > 0 else ""
                    
                    author = comment.get('author', 'Anonymous')
                    time = comment.get('time', '')
                    content = comment.get('content', '')
                    
                    # í—¤ë” (ë²ˆí˜¸, ì‘ì„±ì, ì‹œê°„)
                    header = f"{indent}[{i}] {marker}{author} ({time})"
                    content_parts.append(f"\n{header}")
                    
                    # ë‚´ìš© (ë©€í‹°ë¼ì¸ì¸ ê²½ìš° ë“¤ì—¬ì“°ê¸° ìœ ì§€)
                    content_lines = content.split('\n')
                    for line in content_lines:
                        content_parts.append(f"{indent}    {line}")
            
            content = "\n".join(content_parts)
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = self._extract_og_meta(soup)
            metadata.update(meta_info)
            metadata["original_url"] = original_url
            metadata["comment_count"] = len(comments) if self.include_comments else self._get_comment_count(soup)
            
            # í† í”½ ID ì¶”ì¶œ
            id_match = re.search(r'id=(\d+)', url)
            if id_match:
                metadata["topic_id"] = id_match.group(1)
            
            return CrawledContent(
                title=title,
                content=content,
                url=url,
                platform=self.platform_name,
                metadata=metadata
            )
            
        except Exception as e:
            logger.error(f"Error parsing article content: {e}")
            return None
    
    def crawl_and_save(self, url: str, filename: Optional[str] = None) -> Optional[Path]:
        """
        URLì„ í¬ë¡¤ë§í•˜ê³  ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
        GCS ì—…ë¡œë“œ ì‹œì—ë„ ì´ êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ output_dirì„ ì„ì‹œ ë³€ê²½í•©ë‹ˆë‹¤.
        """
        content = self.extract(url)
        if content is None:
            logger.error(f"Failed to extract content from: {url}")
            return None
        
        # 1. ë‚ ì§œ ê¸°ë°˜ í´ë”ëª… ê³„ì‚°
        pub_time = content.metadata.get("published_time", "")
        folder_name = "unknown_date"
        
        match = re.search(r'(\d{4})-(\d{2})-(\d{2})', pub_time)
        if match:
            folder_name = f"{match.group(1)}_{match.group(2)}_{match.group(3)}"
        else:
            folder_name = datetime.now().strftime("%Y_%m_%d")
            
        # 2. ê²½ë¡œ ì„ì‹œ ë³€ê²½ (BaseCrawlerê°€ ì´ ê²½ë¡œë¥¼ ì°¸ì¡°í•˜ì—¬ ì €ì¥)
        original_output_dir = self.output_dir
        self.output_dir = self.output_dir / folder_name
        
        # 3. ë¡œì»¬ ì €ì¥ì†Œ ìƒì„± (Local ì˜µì…˜ì´ ì¼œì§„ ê²½ìš°ë§Œ)
        if self.save_local:
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # 4. ì €ì¥ ë° ì—…ë¡œë“œ ìœ„ì„ (BaseCrawler)
            # GCS ì—…ë¡œë“œ ì‹œ BaseCrawler êµ¬í˜„ì— ë”°ë¼ 'geeknews_article/YYYY_MM_DD/filename.txt' í˜•íƒœê°€ ë  ìˆ˜ ìˆìŒ
            # (BaseCrawlerê°€ output_dir êµ¬ì¡°ë¥¼ ì–´ë–»ê²Œ GCS keyë¡œ ë§¤í•‘í•˜ëŠëƒì— ë”°ë¼ ë‹¤ë¦„)
            return self.save_to_file(content, filename)
        finally:
            self.output_dir = original_output_dir

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ì•„í‹°í´ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        title_elem = soup.select_one(".topictitle h1")
        if title_elem:
            return self.text_extractor.clean_text(title_elem.get_text(strip=True))
        
        title_link = soup.select_one(".topictitle a.ud")
        if title_link:
            return self.text_extractor.clean_text(title_link.get_text(strip=True))
        
        title_tag = soup.find("title")
        if title_tag:
            title_text = title_tag.get_text(strip=True)
            return re.sub(r'\s*\|\s*GeekNews\s*$', '', title_text)
        return ""
    
    def _extract_original_url(self, soup: BeautifulSoup) -> str:
        """ì›ë³¸ ì™¸ë¶€ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        link_elem = soup.select_one(".topictitle a.ud")
        if link_elem:
            href = link_elem.get("href", "")
            if href and not href.startswith("/") and "news.hada.io" not in href:
                return href
        return ""
    
    def _extract_meta_info(self, soup: BeautifulSoup) -> dict:
        """ë©”íƒ€ ì •ë³´(ì‘ì„±ì, í¬ì¸íŠ¸, ì‹œê°„)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        meta_info = {}
        info_elem = soup.select_one(".topicinfo")
        if info_elem:
            info_text = info_elem.get_text(strip=True)
            
            points_match = re.search(r'(\d+)P', info_text)
            if points_match:
                meta_info["points"] = points_match.group(1)
            
            author_link = info_elem.select_one("a[href*='/user']")
            if author_link:
                meta_info["author"] = author_link.get_text(strip=True)
            
            time_elem = info_elem.select_one("span[title]")
            if time_elem:
                meta_info["published_time"] = time_elem.get("title", "")
            else:
                time_match = re.search(r'(\d+[ì¼ì‹œë¶„ì´ˆ]+ì „)', info_text)
                if time_match:
                    meta_info["relative_time"] = time_match.group(1)
        return meta_info
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        content_elem = soup.select_one(".topic_contents")
        if content_elem:
            inner_content = content_elem.select_one("#topic_contents, span")
            if inner_content:
                return self._format_content(inner_content)
            return self._format_content(content_elem)
        return ""
    
    def _format_content(self, element) -> str:
        """ì½˜í…ì¸ ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        if element is None:
            return ""
        
        for ul in element.find_all("ul"):
            for li in ul.find_all("li"):
                if li.string:
                    li.string = f"â€¢ {li.string}"
                else:
                    li.insert(0, "â€¢ ")
        
        for i in range(1, 7):
            for header in element.find_all(f"h{i}"):
                text = header.get_text(strip=True)
                header.string = f"\n{'#' * i} {text}\n"
        
        for link in element.find_all("a"):
            text = link.get_text(strip=True)
            if text:
                link.string = f"{text}"
        
        for bq in element.find_all("blockquote"):
            text = bq.get_text(strip=True)
            bq.string = f"\n> {text}\n"
        
        for code in element.find_all("code"):
            text = code.get_text(strip=True)
            code.string = f"`{text}`"
        
        text = element.get_text(separator="\n", strip=True)
        return self.text_extractor.clean_text(text)
    
    def _extract_comments(self, soup: BeautifulSoup) -> list[dict]:
        """
        ëŒ“ê¸€ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        HTML êµ¬ì¡°: <div class="comment_row" ...> ... <span class="comment_contents">
        """
        comments = []
        
        # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        comment_thread = soup.select_one("#comment_thread, .comment_thread")
        if comment_thread is None:
            return comments
        
        # ê°œë³„ ëŒ“ê¸€ í–‰ ìˆœíšŒ
        for comment_row in comment_thread.select(".comment_row"):
            comment = {}
            
            # [ë³´ì™„] Depth ì¶”ì¶œ (style="--depth:0")
            style = comment_row.get('style', '')
            depth_match = re.search(r'--depth:(\d+)', style)
            comment['depth'] = int(depth_match.group(1)) if depth_match else 0
            
            # ì‘ì„±ì
            author_elem = comment_row.select_one(".commentinfo a[href*='/user']")
            if author_elem:
                comment["author"] = author_elem.get_text(strip=True)
            
            # ì‹œê°„
            time_elem = comment_row.select_one(".commentinfo a[href*='comment?id']")
            if time_elem:
                comment["time"] = time_elem.get_text(strip=True)
            
            # [ë³´ì™„] ë‚´ìš© ì¶”ì¶œ ê°•í™” (comment_contents í´ë˜ìŠ¤ íƒ€ê²ŸíŒ…)
            # .commentTD > span.comment_contents êµ¬ì¡°
            content_elem = comment_row.select_one(".comment_contents")
            if content_elem:
                # p íƒœê·¸ ë“±ì˜ ì¤„ë°”ê¿ˆì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ separator='\n' ì‚¬ìš©
                raw_text = content_elem.get_text(separator="\n", strip=True)
                comment["content"] = self.text_extractor.clean_text(raw_text)
            
            if comment.get("content"):
                comments.append(comment)
        
        return comments
    
    def _get_comment_count(self, soup: BeautifulSoup) -> int:
        """ëŒ“ê¸€ ìˆ˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        info_elem = soup.select_one(".topicinfo")
        if info_elem:
            comment_link = info_elem.find("a", string=re.compile(r'ëŒ“ê¸€ \d+ê°œ'))
            if comment_link:
                match = re.search(r'(\d+)', comment_link.get_text())
                if match:
                    return int(match.group(1))
        return 0


def main():
    """CLI ì§„ì…ì """
    parser = argparse.ArgumentParser(
        description="GeekNews Article í¬ë¡¤ëŸ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument("url", help="í¬ë¡¤ë§í•  GeekNews Article URL")
    parser.add_argument("--output", "-o", default="geeknews/articles", help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./geeknews/articles)")
    parser.add_argument("--filename", "-f", default=None, help="ì €ì¥í•  íŒŒì¼ëª…")
    parser.add_argument("--comments", "-c", action="store_true", default=True, help="ëŒ“ê¸€ í¬í•¨")
    parser.add_argument("--no-comments", dest="comments", action="store_false", help="ëŒ“ê¸€ ì œì™¸")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")

    # GCS Flags
    parser.add_argument("--gcs", action="store_true", help="Upload to GCS")
    parser.add_argument("--no-local", action="store_true", help="Do not save locally")
    
    args = parser.parse_args()
    
    if not args.verbose:
        logger.remove()
        logger.add(lambda msg: print(msg), level="INFO")
    
    save_local = not args.no_local
    
    # BaseCrawlerë¥¼ ìƒì†ë°›ì•˜ìœ¼ë¯€ë¡œ initì— ì¸ì ì „ë‹¬ ê°€ëŠ¥
    with GeekNewsArticleCrawler(
        output_dir=args.output, 
        save_local=save_local, 
        save_gcs=args.gcs,
        include_comments=args.comments
    ) as crawler:
        filepath = crawler.crawl_and_save(args.url, filename=args.filename)
        
        if filepath:
            print(f"\nâœ… Successfully saved to: {filepath}")
        else:
            print(f"\nâŒ Failed to crawl: {args.url}")
            exit(1)


if __name__ == "__main__":
    main()