"""
Base Crawler Module

í™•ì¥ ê°€ëŠ¥í•œ ì›¹ í¬ë¡¤ëŸ¬ì˜ ê¸°ë³¸ ì¶”ìƒ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
ë‹¤ì–‘í•œ ì›¹ ì†ŒìŠ¤(GeekNews, GitHub, Substack, Turing-Post ë“±)ë¡œ í™•ì¥í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import os
import re
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Optional
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup
from loguru import logger

from gcs_handler import GCSHandler


@dataclass
class CrawledContent:
    """í¬ë¡¤ë§ëœ ì½˜í…ì¸ ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    
    title: str
    content: str
    url: str
    platform: str
    crawled_at: datetime = field(default_factory=datetime.now)
    metadata: dict = field(default_factory=dict)
    
    def to_dict(self) -> dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "title": self.title,
            "content": self.content,
            "url": self.url,
            "platform": self.platform,
            "crawled_at": self.crawled_at.isoformat(),
            "metadata": self.metadata
        }
    
    def to_text(self) -> str:
        """í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        lines = [
            f"{'='*60}",
            f"Title: {self.title}",
            f"URL: {self.url}",
            f"Platform: {self.platform}",
            f"Crawled At: {self.crawled_at.strftime('%Y-%m-%d %H:%M:%S')}",
            f"{'='*60}",
            "",
        ]
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        if self.metadata:
            lines.append("--- Metadata ---")
            for key, value in self.metadata.items():
                lines.append(f"{key}: {value}")
            lines.append("")
        
        lines.append("--- Content ---")
        lines.append(self.content)
        
        return "\n".join(lines)


class BaseCrawler(ABC):
    """
    ëª¨ë“  ì›¹ í¬ë¡¤ëŸ¬ì˜ ê¸°ë³¸ ì¶”ìƒ í´ë˜ìŠ¤
    
    ìƒˆë¡œìš´ í¬ë¡¤ëŸ¬ë¥¼ ë§Œë“¤ ë•Œ ì´ í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    
    # í´ë˜ìŠ¤ ë³€ìˆ˜: í”Œë«í¼ ì‹ë³„ì
    platform_name: str = "base"
    
    # ê¸°ë³¸ ìš”ì²­ í—¤ë”
    DEFAULT_HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    }

    # GCS ì„¤ì •
    GCS_PROJECT_ID = "gen-lang-client-0039052673"
    GCS_BUCKET_NAME = "parallel_audio_etl_data"
    
    def __init__(
        self,
        output_dir: str = "./output",
        timeout: int = 30,
        headers: Optional[dict] = None,
        save_local: bool = True,
        save_gcs: bool = False
    ):
        """
        Args:
            output_dir: ì¶œë ¥ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬
            timeout: HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            headers: ì»¤ìŠ¤í…€ HTTP í—¤ë”
        """
        self.output_dir = Path(output_dir)
        self.timeout = timeout
        self.headers = headers or self.DEFAULT_HEADERS.copy()
        self.session = requests.Session()
        self.session.headers.update(self.headers)

        self.save_local = save_local
        self.save_gcs = save_gcs
        
        self.gcs_handler = None
        if self.save_gcs:
            self.gcs_handler = GCSHandler(self.GCS_PROJECT_ID, self.GCS_BUCKET_NAME)
            logger.info(f"GCS Upload Enabled: gs://{self.GCS_BUCKET_NAME}")

        if self.save_local:
            self.output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Local Storage Enabled: {self.output_dir}")
    
    def fetch_html(self, url: str) -> Optional[str]:
        """
        URLì—ì„œ HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            url: í¬ë¡¤ë§í•  URL
            
        Returns:
            HTML ë¬¸ìì—´ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        try:
            logger.info(f"Fetching HTML from: {url}")
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'
            return response.text
        except requests.RequestException as e:
            logger.error(f"Failed to fetch HTML from {url}: {e}")
            return None
    
    def parse_html(self, html: str) -> BeautifulSoup:
        """
        HTMLì„ BeautifulSoup ê°ì²´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            html: HTML ë¬¸ìì—´
            
        Returns:
            BeautifulSoup ê°ì²´
        """
        return BeautifulSoup(html, "html.parser")
    
    @abstractmethod
    def extract(self, url: str) -> Optional[CrawledContent]:
        """
        URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            url: í¬ë¡¤ë§í•  URL
            
        Returns:
            CrawledContent ê°ì²´ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        pass
    
    @abstractmethod
    def _parse_content(self, soup: BeautifulSoup, url: str) -> Optional[CrawledContent]:
        """
        BeautifulSoup ê°ì²´ì—ì„œ ì½˜í…ì¸ ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            soup: BeautifulSoup ê°ì²´
            url: ì›ë³¸ URL
            
        Returns:
            CrawledContent ê°ì²´ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        pass
    
    def save_to_file(
        self,
        content: "CrawledContent", 
        filename: Optional[str] = None
    ) -> Optional[Path]:
        """
        í¬ë¡¤ë§ëœ ì½˜í…ì¸ ë¥¼ ì €ì¥í•©ë‹ˆë‹¤ (Local ë° GCS ì¤‘ë³µ ì²´í¬ í¬í•¨).
        """
        if filename is None:
            safe_title = self._sanitize_filename(content.title)
            
            # 1. ê³ ìœ  ID í™•ì¸ (Topic ID, Week ID ë“±)
            unique_id = content.metadata.get("topic_id") or content.metadata.get("week_id")
            
            if unique_id:
                # IDê°€ ìˆìœ¼ë©´ IDë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ ì„± ë³´ì¥ (ì˜ˆ: geeknews_article_24460_Title.txt)
                filename = f"{content.platform}_{unique_id}_{safe_title}.txt"
            else:
                # 2. IDê°€ ì—†ìœ¼ë©´ ë‚ ì§œ(YYYYMMDD)ë§Œ ì‚¬ìš© (í•˜ë£¨ì— í•œ ë²ˆë§Œ ì €ì¥ë˜ë„ë¡)
                date_tag = datetime.now().strftime("%Y%m%d")
                filename = f"{content.platform}_{safe_title}_{date_tag}.txt"
        
        text_content = content.to_text()
        saved_path = None

        # 1. Local Save (ì¤‘ë³µ ì²´í¬)
        if self.save_local:
            saved_path = self.output_dir / filename
            if saved_path.exists():
                logger.info(f"â­ï¸  Skipped local save (Duplicate): {saved_path}")
            else:
                try:
                    with open(saved_path, "w", encoding="utf-8") as f:
                        f.write(text_content)
                    logger.info(f"ğŸ’¾ Saved locally to: {saved_path}")
                except Exception as e:
                    logger.error(f"âŒ Failed to save locally: {e}")

        # 2. GCS Upload (ì¤‘ë³µ ì²´í¬)
        if self.save_gcs and self.gcs_handler:
            try:
                current_folder_name = self.output_dir.name
                gcs_path = f"{content.platform}/{current_folder_name}/{filename}"
                
                # GCS ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                if self.gcs_handler.file_exists(gcs_path):
                    logger.info(f"â­ï¸  Skipped GCS upload (Duplicate): gs://{self.gcs_handler.bucket_name}/{gcs_path}")
                else:
                    self.gcs_handler.upload_string(text_content, gcs_path)
            except Exception as e:
                logger.error(f"âŒ GCS Upload failed logic: {e}")

        # ë¡œì»¬ ì €ì¥ì„ ì•ˆí•˜ë”ë¼ë„(saved_pathê°€ ì—†ì–´ë„), ë…¼ë¦¬ì ì¸ íŒŒì¼ ê²½ë¡œëŠ” í˜¸ì¶œìì—ê²Œ í•„ìš”í•  ìˆ˜ ìˆìŒ
        if not saved_path:
             saved_path = self.output_dir / filename

        return saved_path

    def crawl_and_save(self, url: str, filename: Optional[str] = None) -> Optional[Path]:
        """
        URLì„ í¬ë¡¤ë§í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            url: í¬ë¡¤ë§í•  URL
            filename: ì €ì¥í•  íŒŒì¼ëª… (ì„ íƒ)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        content = self.extract(url)
        if content is None:
            logger.error(f"Failed to extract content from: {url}")
            return None
        
        return self.save_to_file(content, filename)
    
    @staticmethod
    def _sanitize_filename(text: str, max_length: int = 50) -> str:
        """
        íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ê¸¸ì´
            
        Returns:
            ì •ì œëœ íŒŒì¼ëª…
        """
        # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
        sanitized = re.sub(r'[<>:"/\\|?*\n\r\t]', '', text)
        # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
        sanitized = re.sub(r'\s+', '_', sanitized)
        # ê¸¸ì´ ì œí•œ
        return sanitized[:max_length].strip('_')
    
    @staticmethod
    def get_domain(url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        parsed = urlparse(url)
        return parsed.netloc
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.session.close()


class BaseTextExtractor:
    """HTMLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""
        
        # ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        # ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.split('\n')]
        return '\n'.join(lines).strip()
    
    @staticmethod
    def extract_text_from_element(element, separator: str = "\n") -> str:
        """
        BeautifulSoup ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            element: BeautifulSoup ìš”ì†Œ
            separator: í…ìŠ¤íŠ¸ êµ¬ë¶„ì
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        if element is None:
            return ""
        return element.get_text(separator=separator, strip=True)