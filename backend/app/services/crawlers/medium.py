"""
Medium Article Crawler

Medium ì•„í‹°í´ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
BaseCrawlerë¥¼ ìƒì†ë°›ì•„ FastAPI ë¹„ë™ê¸° íŒ¨í„´ê³¼ í˜¸í™˜ë©ë‹ˆë‹¤.

ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì „ì²´ ì½˜í…ì¸ (í˜ì´ì›” í¬í•¨)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
- Freedium: https://freedium.cfd/{medium_url}
- Scribe.rip: https://scribe.rip/{path}
- trafilatura fallback: ì›ë³¸ URLì—ì„œ ì§ì ‘ ì¶”ì¶œ

URL í˜•ì‹:
- https://medium.com/@username/article-title-xxxxx
- https://medium.com/publication/article-title-xxxxx
- ì»¤ìŠ¤í…€ ë„ë©”ì¸ (meta íƒœê·¸ë¡œ Medium ì—¬ë¶€ í™•ì¸)

Usage:
    crawler = MediumCrawler()
    article = await crawler.extract("https://medium.com/@user/article")
"""

import asyncio
import json
import re
from urllib.parse import urlparse

import trafilatura
from bs4 import BeautifulSoup
from loguru import logger
from playwright.async_api import TimeoutError as PlaywrightTimeout
from playwright.async_api import async_playwright

from app.services.crawlers.base import BaseCrawler, BaseTextExtractor
from app.services.crawlers.schemas import CrawledArticle


class MediumTextExtractor(BaseTextExtractor):
    """
    Medium/Freedium í˜ì´ì§€ íŠ¹í™” í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°

    ë¶ˆí•„ìš”í•œ UI ìš”ì†Œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """

    REMOVE_SELECTORS = [
        "script",
        "style",
        "noscript",
        "iframe",
        "nav",
        "footer",
        "button",
        "header",
        ".sidebar",
        ".ad",
        ".advertisement",
        "[data-testid='headerSignUpButton']",
        "[data-testid='headerSignInButton']",
        ".speechify-ignore",
        ".grecaptcha-badge",
    ]

    # Freedium ë…¸ì´ì¦ˆ í…ìŠ¤íŠ¸ íŒ¨í„´ (ì´ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ìš”ì†Œì™€ ê·¸ ì´í›„ í˜•ì œ ìš”ì†Œë¥¼ ì œê±°)
    FREEDIUM_NOISE_TEXTS = [
        "Reporting a Problem",
        "Sometimes we have problems displaying some Medium posts",
        "fucking Cloudflare",
    ]

    def clean_html(self, soup: BeautifulSoup) -> BeautifulSoup:
        """HTMLì—ì„œ ë…¸ì´ì¦ˆ ìš”ì†Œë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        soup_copy = BeautifulSoup(str(soup), "html.parser")

        # 1. ì…€ë ‰í„° ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
        for selector in self.REMOVE_SELECTORS:
            for element in soup_copy.select(selector):
                element.decompose()

        # 2. Freedium í…ìŠ¤íŠ¸ ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
        for noise_text in self.FREEDIUM_NOISE_TEXTS:
            for tag in soup_copy.find_all(["h1", "h2", "h3", "p"]):
                if noise_text in tag.get_text():
                    # í•´ë‹¹ íƒœê·¸ì™€ ê·¸ ë’¤ì˜ ëª¨ë“  í˜•ì œ ìš”ì†Œ ì œê±°
                    for sibling in list(tag.find_next_siblings()):
                        sibling.decompose()
                    tag.decompose()
                    break  # í•´ë‹¹ íŒ¨í„´ì€ í•œ ë²ˆë§Œ ì œê±°

        return soup_copy


class MediumCrawler(BaseCrawler):
    """
    Medium Article í¬ë¡¤ëŸ¬ (Multi-Mirror ê¸°ë°˜)

    ì—¬ëŸ¬ ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ Medium ì•„í‹°í´ì˜ ì „ì²´ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - Freedium: ë´‡ íƒì§€ ìš°íšŒ, í˜ì´ì›” ì½˜í…ì¸  ì ‘ê·¼
    - Scribe.rip: Freedium ëŒ€ì•ˆ, ê¹”ë”í•œ HTML êµ¬ì¡°
    - trafilatura: ìµœí›„ì˜ fallback, ì›ë³¸ URLì—ì„œ ì§ì ‘ ì¶”ì¶œ

    ê°œë³„ ì•„í‹°í´ í˜ì´ì§€ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤:
    - ì œëª© ë° ë¶€ì œëª©
    - ì‘ì„±ì ì •ë³´
    - ê²Œì‹œì¼ ë° ì½ëŠ” ì‹œê°„
    - ë³¸ë¬¸ ë‚´ìš© (ì½”ë“œë¸”ë¡, ì¸ìš©êµ¬, ë¦¬ìŠ¤íŠ¸ ë³´ì¡´)
    """

    platform_name: str = "medium"

    # ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ ëª©ë¡ (ìš°ì„ ìˆœìœ„ ìˆœ)
    MIRROR_SERVICES: list[tuple[str, str]] = [
        ("freedium", "https://freedium.cfd"),
        ("scribe", "https://scribe.rip"),
    ]

    # Freedium ë¯¸ëŸ¬ ì‚¬ì´íŠ¸ URL (í˜¸í™˜ì„± ìœ ì§€)
    FREEDIUM_BASE_URL: str = "https://freedium.cfd"

    # Medium í‘œì¤€ URL íŒ¨í„´
    URL_PATTERNS: list[str] = [
        r"https?://(www\.)?medium\.com/.+",
        r"https?://[a-zA-Z0-9-]+\.medium\.com/.+",
    ]

    # HTTP í—¤ë”
    DEFAULT_HEADERS: dict = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9,ko;q=0.8",
    }

    # ê¸°ë³¸ ìš”ì²­ ì§€ì—° (ì´ˆ) - Rate limiting ë°©ì§€
    DEFAULT_REQUEST_DELAY: float = 0.5

    def __init__(
        self,
        timeout: float | None = None,
        headers: dict | None = None,
        request_delay: float | None = None,
        use_freedium: bool = True,
    ):
        """
        Args:
            timeout: HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ). ê¸°ë³¸ê°’ 30ì´ˆ
            headers: ì»¤ìŠ¤í…€ HTTP í—¤ë”
            request_delay: ìš”ì²­ ì „ ì§€ì—° ì‹œê°„ (ì´ˆ). ê¸°ë³¸ê°’ 0.5ì´ˆ
            use_freedium: Freedium ë¯¸ëŸ¬ ì‚¬ì´íŠ¸ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’ True)
        """
        super().__init__(timeout=timeout, headers=headers or self.DEFAULT_HEADERS)
        self.text_extractor = MediumTextExtractor()
        self.request_delay = (
            request_delay if request_delay is not None else self.DEFAULT_REQUEST_DELAY
        )
        self.use_freedium = use_freedium

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # URL ë³€í™˜ ë° ê²€ì¦
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _convert_to_mirror_url(self, url: str, service: str) -> str:
        """
        Medium URLì„ ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            url: ì›ë³¸ Medium URL
            service: ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ ì´ë¦„ ('freedium', 'scribe')

        Returns:
            ë³€í™˜ëœ ë¯¸ëŸ¬ URL
        """
        parsed = urlparse(url)
        path = parsed.path  # /@username/article-title-xxx

        if service == "freedium":
            return f"https://freedium.cfd/{url}"
        elif service == "scribe":
            # Scribe.ripì€ ê²½ë¡œë§Œ ì‚¬ìš©
            # https://medium.com/@user/article -> https://scribe.rip/@user/article
            return f"https://scribe.rip{path}"

        return url

    def _convert_to_freedium_url(self, url: str) -> str:
        """
        Medium URLì„ Freedium URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (í˜¸í™˜ì„± ìœ ì§€)

        Args:
            url: ì›ë³¸ Medium URL

        Returns:
            Freedium URL (ì˜ˆ: https://freedium.cfd/https://medium.com/...)
        """
        return self._convert_to_mirror_url(url, "freedium")

    def _extract_original_url(self, mirror_url: str) -> str:
        """
        ë¯¸ëŸ¬ URLì—ì„œ ì›ë³¸ Medium URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            mirror_url: Freedium ë˜ëŠ” Scribe URL

        Returns:
            ì›ë³¸ Medium URL
        """
        # Freedium URL ì²˜ë¦¬
        if mirror_url.startswith(self.FREEDIUM_BASE_URL + "/"):
            return mirror_url[len(self.FREEDIUM_BASE_URL) + 1 :]

        # Scribe URL ì²˜ë¦¬
        if mirror_url.startswith("https://scribe.rip/"):
            path = mirror_url[len("https://scribe.rip") :]
            return f"https://medium.com{path}"

        return mirror_url

    def validate_url(self, url: str) -> bool:
        """
        URLì´ Medium ì•„í‹°í´ URLì¸ì§€ ê²€ì¦í•©ë‹ˆë‹¤.

        Args:
            url: ê²€ì¦í•  URL

        Returns:
            ìœ íš¨í•œ Medium URLì´ë©´ True
        """
        # Freedium URLì¸ ê²½ìš° ì›ë³¸ URL ì¶”ì¶œ
        check_url = self._extract_original_url(url)

        for pattern in self.URL_PATTERNS:
            if re.match(pattern, check_url):
                return True
        return False

    def _parse_content(self, soup: BeautifulSoup, url: str) -> CrawledArticle | None:
        """
        BeautifulSoupì—ì„œ ì•„í‹°í´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        BaseCrawler ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„.
        ê¸°ë³¸ì ìœ¼ë¡œ Freedium íŒŒì‹± ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            soup: BeautifulSoup ê°ì²´
            url: ì›ë³¸ URL

        Returns:
            CrawledArticle ê°ì²´ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        return self._parse_freedium_content(soup, url)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í¬ë¡¤ë§ ë©”ì„œë“œ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def extract(self, url: str) -> CrawledArticle | None:
        """
        Medium ì•„í‹°í´ URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ì „ì²´ í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸:
        1. validate_url()ë¡œ URL ê²€ì¦
        2. ìš”ì²­ ì§€ì—° (rate limiting ë°©ì§€)
        3. ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ ìˆœì°¨ ì‹œë„ (Freedium â†’ Scribe.rip)
        4. ëª¨ë“  ë¯¸ëŸ¬ ì‹¤íŒ¨ ì‹œ trafilatura fallback
        5. _parse_content()ë¡œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ

        Args:
            url: í¬ë¡¤ë§í•  Medium ì•„í‹°í´ URL

        Returns:
            CrawledArticle ê°ì²´ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        # ì›ë³¸ URL ì €ì¥ (ê²°ê³¼ì— ì‚¬ìš©)
        original_url = self._extract_original_url(url)

        # URL íŒ¨í„´ ê²€ì‚¬
        if not self.validate_url(url):
            logger.warning(f"URL pattern doesn't match Medium format: {url}")

        # ìš”ì²­ ì§€ì—° (rate limiting ë°©ì§€)
        if self.request_delay > 0:
            logger.debug(f"Request delay: {self.request_delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
            await asyncio.sleep(self.request_delay)

        # ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ ì‚¬ìš©ì´ í™œì„±í™”ëœ ê²½ìš°
        if self.use_freedium:
            # ê° ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ ìˆœì°¨ ì‹œë„
            for service_name, _base_url in self.MIRROR_SERVICES:
                mirror_url = self._convert_to_mirror_url(original_url, service_name)
                logger.info(f"{service_name}ì„ í†µí•´ ì‹œë„: {mirror_url}")

                html = await self.fetch_html(mirror_url)

                # HTMLì´ ìœ íš¨í•œì§€ ê²€ì¦ (ìµœì†Œ ê¸¸ì´, ì—ëŸ¬ í˜ì´ì§€ ì•„ë‹˜)
                if html and len(html) > 1000 and not self._is_error_page(html):
                    logger.info(f"âœ… {service_name} ì„±ê³µ! ({len(html):,} bytes)")
                    soup = self.parse_html(html)

                    # ì„œë¹„ìŠ¤ë³„ íŒŒì‹± ë¡œì§ ë¶„ê¸°
                    if service_name == "freedium":
                        result = self._parse_freedium_content(soup, original_url)
                    elif service_name == "scribe":
                        result = self._parse_scribe_content(soup, original_url)
                    else:
                        result = self._parse_freedium_content(soup, original_url)

                    if result and len(result.content) > 100:
                        return result
                    else:
                        logger.warning(
                            f"{service_name} íŒŒì‹± ê²°ê³¼ ë¶ˆì¶©ë¶„, ë‹¤ìŒ ì„œë¹„ìŠ¤ ì‹œë„..."
                        )
                else:
                    logger.warning(f"âŒ {service_name} ì‹¤íŒ¨, ë‹¤ìŒ ì„œë¹„ìŠ¤ ì‹œë„...")

        # ëª¨ë“  ë¯¸ëŸ¬ ì‹¤íŒ¨ ì‹œ trafilatura fallback
        logger.info("ëª¨ë“  ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ ì‹¤íŒ¨, trafilatura fallback ì‹œë„...")
        result = await self._extract_with_trafilatura(original_url)
        if result:
            return result

        # ìµœí›„ì˜ ìˆ˜ë‹¨ 1: ì›ë³¸ Medium URLì—ì„œ ì§ì ‘ íŒŒì‹±
        logger.info(f"trafilatura ì‹¤íŒ¨, ì›ë³¸ Medium URL ì§ì ‘ íŒŒì‹± ì‹œë„: {original_url}")
        html = await self.fetch_html(original_url)
        if html:
            soup = self.parse_html(html)
            result = self._parse_medium_content(soup, original_url)
            if result and len(result.content) > 100:
                return result

        # ìµœí›„ì˜ ìˆ˜ë‹¨ 2: Playwright ë™ì  ë Œë”ë§
        logger.info("ëª¨ë“  ì •ì  ë°©ë²• ì‹¤íŒ¨, Playwright ë™ì  ë Œë”ë§ ì‹œë„...")
        result = await self._extract_with_playwright(original_url)
        if result:
            return result

        logger.error(f"ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ (ë¯¸ëŸ¬ + trafilatura + Playwright): {url}")
        return None

    def _is_error_page(self, html: str) -> bool:
        """HTMLì´ ì—ëŸ¬ í˜ì´ì§€ì¸ì§€ í™•ì¸"""
        error_indicators = [
            "404 Not Found",
            "Page not found",
            "Error 404",
            "We couldn't find",
            "This page doesn't exist",
            "Access denied",
            "403 Forbidden",
            "PAGE NOT FOUND",  # Medium 404 í˜ì´ì§€
            "Out of nothing, something",  # Medium 404 í˜ì´ì§€ ë¬¸êµ¬
        ]
        html_lower = html.lower()
        return any(indicator.lower() in html_lower for indicator in error_indicators)

    def _is_404_content(self, content: str) -> bool:
        """ì¶”ì¶œëœ ì½˜í…ì¸ ê°€ 404 í˜ì´ì§€ ë‚´ìš©ì¸ì§€ í™•ì¸"""
        if not content:
            return True

        content_lower = content.lower()
        error_indicators = [
            "page not found",
            "404",
            "out of nothing, something",  # Medium 404 í˜ì´ì§€ íŠ¹ìœ  ë¬¸êµ¬
            "you can find (just about) anything on medium",
        ]

        # ì²˜ìŒ 500ìì— ì—ëŸ¬ í‘œì‹œê°€ ìˆìœ¼ë©´ 404 í˜ì´ì§€
        first_part = content_lower[:500]
        return any(indicator in first_part for indicator in error_indicators)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Freedium íŒŒì‹±
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _parse_freedium_content(
        self, soup: BeautifulSoup, url: str
    ) -> CrawledArticle | None:
        """
        Freedium HTMLì—ì„œ ì•„í‹°í´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Freediumì€ Medium ì½˜í…ì¸ ë¥¼ ì •ì œëœ í˜•íƒœë¡œ ì œê³µí•©ë‹ˆë‹¤.
        """
        try:
            # ë…¸ì´ì¦ˆ ì œê±°
            clean_soup = self.text_extractor.clean_html(soup)

            # ì œëª© ì¶”ì¶œ
            title = self._extract_freedium_title(clean_soup)

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            meta_info = self._extract_freedium_metadata(clean_soup)

            # ë³¸ë¬¸ ì¶”ì¶œ
            article_body = self._extract_freedium_body(clean_soup)

            # ì „ì²´ ì½˜í…ì¸  ì¡°í•©
            content = self._build_content(meta_info, article_body)

            # ArticleMetadata ìƒì„±
            metadata = self._build_metadata(
                {},  # OG ë©”íƒ€ëŠ” Freediumì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ
                author=meta_info.get("author"),
                published_at=meta_info.get("date"),
                read_time=meta_info.get("read_time"),
                subtitle=meta_info.get("subtitle"),
            )

            return CrawledArticle(
                title=title or "Untitled Medium Article",
                content=content,
                url=url,
                platform=self.platform_name,
                metadata=metadata,
                secondary_urls=[],
            )

        except Exception as e:
            logger.error(f"Error parsing Freedium content: {e}")
            return None

    def _extract_freedium_title(self, soup: BeautifulSoup) -> str | None:
        """Freediumì—ì„œ ì œëª© ì¶”ì¶œ"""
        # h1 íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
        title_elem = soup.select_one("h1")
        if title_elem:
            return self.text_extractor.clean_text(title_elem.get_text())

        # fallback: title íƒœê·¸
        title_tag = soup.find("title")
        if title_tag:
            title_text = title_tag.get_text()
            # " - Freedium" ì ‘ë¯¸ì‚¬ ì œê±°
            if " - Freedium" in title_text:
                title_text = title_text.replace(" - Freedium", "")
            return self.text_extractor.clean_text(title_text)

        return None

    def _extract_freedium_metadata(self, soup: BeautifulSoup) -> dict:
        """Freediumì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        meta = {}

        # ì‘ì„±ì (ë³´í†µ ì²« ë²ˆì§¸ ë§í¬ë‚˜ íŠ¹ì • í´ë˜ìŠ¤)
        author_elem = soup.select_one('.author, [rel="author"], a[href*="/@"]')
        if author_elem:
            meta["author"] = self.text_extractor.clean_text(author_elem.get_text())

        # ë¶€ì œëª© (h1 ë‹¤ìŒì˜ h2 ë˜ëŠ” íŠ¹ì • í´ë˜ìŠ¤)
        subtitle_elem = soup.select_one("h2, .subtitle")
        if subtitle_elem:
            subtitle_text = self.text_extractor.clean_text(subtitle_elem.get_text())
            # ë¶€ì œëª©ì´ ë„ˆë¬´ ê¸¸ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì‚¬ìš© (ë³¸ë¬¸ h2ì™€ êµ¬ë¶„)
            if len(subtitle_text) < 200:
                meta["subtitle"] = subtitle_text

        # ë‚ ì§œ ì¶”ì¶œ (time íƒœê·¸ ë˜ëŠ” ë‚ ì§œ íŒ¨í„´)
        time_elem = soup.select_one("time")
        if time_elem:
            meta["date"] = time_elem.get("datetime") or time_elem.get_text(strip=True)

        return meta

    def _extract_freedium_body(self, soup: BeautifulSoup) -> str:
        """Freediumì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""
        # Freediumì€ main ë˜ëŠ” article íƒœê·¸ì— ë³¸ë¬¸ì„ ë„£ìŒ
        article_content = (
            soup.select_one("main")
            or soup.select_one("article")
            or soup.select_one(".content")
            or soup.select_one("body")
        )

        if not article_content:
            return ""

        content_parts = []

        # ëª¨ë“  ì˜ë¯¸ ìˆëŠ” íƒœê·¸ ìˆœíšŒ
        tags = article_content.find_all(
            [
                "h1",
                "h2",
                "h3",
                "h4",
                "h5",
                "h6",
                "p",
                "blockquote",
                "pre",
                "ul",
                "ol",
                "figure",
                "img",
            ]
        )

        seen_texts = set()  # ì¤‘ë³µ ì œê±°ìš©

        for tag in tags:
            text_content = self._format_tag(tag)
            if text_content and text_content not in seen_texts:
                content_parts.append(text_content)
                seen_texts.add(text_content)

        return "\n".join(content_parts)

    def _format_tag(self, tag) -> str | None:
        """íƒœê·¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if tag.name == "figure" or tag.name == "img":
            img = tag if tag.name == "img" else tag.find("img")
            if img:
                alt_text = img.get("alt", "")
                src = img.get("src", "")
                caption = tag.find("figcaption") if tag.name == "figure" else None
                caption_text = caption.get_text(strip=True) if caption else ""

                result = f"\n[Image: {alt_text}]({src})"
                if caption_text:
                    result += f"\n  â””â”€ <caption>: {caption_text}"
                return result
            return None

        if tag.name == "pre":
            code_text = tag.get_text(separator="\n", strip=True)
            return f"\n```\n{code_text}\n```\n"

        if tag.name == "blockquote":
            quote_text = self.text_extractor.clean_text(tag.get_text())
            return f"\n> {quote_text}\n"

        if tag.name in ["ul", "ol"]:
            items = []
            for idx, li in enumerate(tag.find_all("li", recursive=False), 1):
                marker = "-" if tag.name == "ul" else f"{idx}."
                li_text = self.text_extractor.clean_text(li.get_text())
                items.append(f"{marker} {li_text}")
            return "\n".join(items) + "\n" if items else None

        if tag.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
            level = int(tag.name[1])
            text = self.text_extractor.clean_text(tag.get_text())
            return f"\n{'#' * level} {text}\n" if text else None

        # ì¼ë°˜ ë¬¸ë‹¨ (p)
        text = self.text_extractor.clean_text(tag.get_text())
        return text if text else None

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Medium ì›ë³¸ íŒŒì‹± (Fallback)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _parse_medium_content(
        self, soup: BeautifulSoup, url: str
    ) -> CrawledArticle | None:
        """
        ì›ë³¸ Medium HTMLì—ì„œ ì•„í‹°í´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (Fallback).

        Mediumì€ JavaScript ë Œë”ë§ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì¼ë¶€ ì½˜í…ì¸ ë§Œ ì¶”ì¶œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        try:
            clean_soup = self.text_extractor.clean_html(soup)

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            meta_info = self._extract_medium_metadata(clean_soup)

            # ë³¸ë¬¸ ì¶”ì¶œ
            article_body = self._extract_medium_body(clean_soup)

            # ì „ì²´ ì½˜í…ì¸  ì¡°í•©
            content = self._build_content(meta_info, article_body)

            # ì œëª© ê²°ì •
            title = meta_info.get("title", "Untitled Medium Article")

            # OG ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            og_meta = self.extract_og_meta(soup)

            # ArticleMetadata ìƒì„±
            metadata = self._build_metadata(
                og_meta,
                author=meta_info.get("author"),
                published_at=meta_info.get("date"),
                read_time=meta_info.get("read_time"),
                claps=meta_info.get("claps"),
                tags=meta_info.get("tags"),
                subtitle=meta_info.get("subtitle"),
            )

            return CrawledArticle(
                title=title,
                content=content,
                url=url,
                platform=self.platform_name,
                metadata=metadata,
                secondary_urls=[],
            )

        except Exception as e:
            logger.error(f"Error parsing Medium content: {e}")
            return None

    def _extract_medium_metadata(self, soup: BeautifulSoup) -> dict:
        """ì›ë³¸ Mediumì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        meta = {}

        # ì œëª©
        title_elem = soup.select_one('[data-testid="storyTitle"]') or soup.select_one(
            "h1"
        )
        if title_elem:
            meta["title"] = self.text_extractor.clean_text(title_elem.get_text())
        else:
            title_tag = soup.find("title")
            if title_tag:
                meta["title"] = self.text_extractor.clean_text(title_tag.get_text())

        # ë¶€ì œëª©
        subtitle_elem = soup.select_one(".pw-subtitle-paragraph")
        if subtitle_elem:
            meta["subtitle"] = self.text_extractor.clean_text(subtitle_elem.get_text())

        # ì‘ì„±ì
        author_elem = soup.select_one('[data-testid="authorName"]')
        if author_elem:
            meta["author"] = self.text_extractor.clean_text(author_elem.get_text())

        # ê²Œì‹œì¼
        date_elem = soup.select_one('[data-testid="storyPublishDate"]')
        if date_elem:
            meta["date"] = self.text_extractor.clean_text(date_elem.get_text())

        # ì½ëŠ” ì‹œê°„
        read_time_elem = soup.select_one('[data-testid="storyReadTime"]')
        if read_time_elem:
            meta["read_time"] = self.text_extractor.clean_text(
                read_time_elem.get_text()
            )

        # ë°•ìˆ˜ ìˆ˜
        clap_elem = soup.select_one(
            '[data-testid="headerClapButton"]'
        ) or soup.select_one('[data-testid="footerClapButton"]')
        if clap_elem:
            clap_text = clap_elem.get_text(strip=True)
            meta["claps"] = re.sub(r"[^0-9K.]", "", clap_text)

        # JSON-LDì—ì„œ íƒœê·¸ ì¶”ì¶œ
        script_json = soup.find("script", type="application/ld+json")
        if script_json and script_json.string:
            try:
                data = json.loads(script_json.string)
                if isinstance(data, dict) and "keywords" in data:
                    keywords = data["keywords"]
                    if isinstance(keywords, list):
                        meta["tags"] = keywords
                    elif isinstance(keywords, str):
                        meta["tags"] = [k.strip() for k in keywords.split(",")]
            except json.JSONDecodeError:
                pass

        return meta

    def _extract_medium_body(self, soup: BeautifulSoup) -> str:
        """ì›ë³¸ Mediumì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""
        article_content = soup.select_one("section") or soup.select_one("article")

        if not article_content:
            return ""

        content_parts = []

        tags = article_content.find_all(
            [
                "h1",
                "h2",
                "h3",
                "h4",
                "h5",
                "h6",
                "p",
                "blockquote",
                "pre",
                "ul",
                "ol",
                "figure",
            ]
        )

        for tag in tags:
            text_content = self._format_tag(tag)
            if text_content:
                content_parts.append(text_content)

        return "\n".join(content_parts)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ê³µí†µ ìœ í‹¸ë¦¬í‹°
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_content(self, meta_info: dict, article_body: str) -> str:
        """ì¶”ì¶œëœ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ì½˜í…ì¸  ë¬¸ìì—´ë¡œ ì¡°í•©"""
        content_parts = []

        # ë¶€ì œëª©
        if meta_info.get("subtitle"):
            content_parts.append(f"ğŸ“ Subtitle: {meta_info['subtitle']}")
            content_parts.append("")

        # ë©”íƒ€ ì •ë³´ ë¼ì¸
        info_items = []
        if meta_info.get("author"):
            info_items.append(f"Author: {meta_info['author']}")
        if meta_info.get("date"):
            info_items.append(f"Date: {meta_info['date']}")
        if meta_info.get("read_time"):
            info_items.append(f"Read Time: {meta_info['read_time']}")
        if meta_info.get("claps"):
            info_items.append(f"ğŸ‘ Claps: {meta_info['claps']}")

        if info_items:
            content_parts.append(" | ".join(info_items))
            content_parts.append("-" * 40)
            content_parts.append("")

        # ë³¸ë¬¸
        if article_body:
            content_parts.append(article_body)

        # íƒœê·¸
        if meta_info.get("tags"):
            content_parts.append("")
            content_parts.append("-" * 40)
            content_parts.append(f"ğŸ·ï¸ Tags: {', '.join(meta_info['tags'])}")

        return "\n".join(content_parts)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Scribe.rip íŒŒì‹±
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _parse_scribe_content(
        self, soup: BeautifulSoup, url: str
    ) -> CrawledArticle | None:
        """
        Scribe.rip HTMLì—ì„œ ì•„í‹°í´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Scribe.ripì€ ê¹”ë”í•œ HTML êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """
        try:
            # ë…¸ì´ì¦ˆ ì œê±°
            clean_soup = self.text_extractor.clean_html(soup)

            # ì œëª© ì¶”ì¶œ
            title = self._extract_scribe_title(clean_soup)

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            meta_info = self._extract_scribe_metadata(clean_soup)

            # ë³¸ë¬¸ ì¶”ì¶œ
            article_body = self._extract_scribe_body(clean_soup)

            # ì „ì²´ ì½˜í…ì¸  ì¡°í•©
            content = self._build_content(meta_info, article_body)

            # ArticleMetadata ìƒì„±
            metadata = self._build_metadata(
                {},
                author=meta_info.get("author"),
                published_at=meta_info.get("date"),
                read_time=meta_info.get("read_time"),
                subtitle=meta_info.get("subtitle"),
            )

            return CrawledArticle(
                title=title or "Untitled Medium Article",
                content=content,
                url=url,
                platform=self.platform_name,
                metadata=metadata,
                secondary_urls=[],
            )

        except Exception as e:
            logger.error(f"Error parsing Scribe content: {e}")
            return None

    def _extract_scribe_title(self, soup: BeautifulSoup) -> str | None:
        """Scribe.ripì—ì„œ ì œëª© ì¶”ì¶œ"""
        # article ë‚´ì˜ h1 ìš°ì„ 
        article = soup.select_one("article")
        if article:
            title_elem = article.select_one("h1")
            if title_elem:
                return self.text_extractor.clean_text(title_elem.get_text())

        # fallback: ì „ì²´ì—ì„œ h1
        title_elem = soup.select_one("h1")
        if title_elem:
            return self.text_extractor.clean_text(title_elem.get_text())

        # fallback: title íƒœê·¸
        title_tag = soup.find("title")
        if title_tag:
            return self.text_extractor.clean_text(title_tag.get_text())

        return None

    def _extract_scribe_metadata(self, soup: BeautifulSoup) -> dict:
        """Scribe.ripì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        meta = {}

        # ì‘ì„±ì (a íƒœê·¸ì—ì„œ @username íŒ¨í„´)
        author_links = soup.select('a[href*="/@"]')
        for link in author_links:
            text = link.get_text(strip=True)
            if text and not text.startswith("http"):
                meta["author"] = text
                break

        # ë‚ ì§œ (time íƒœê·¸ ë˜ëŠ” datetime ì†ì„±)
        time_elem = soup.select_one("time")
        if time_elem:
            meta["date"] = time_elem.get("datetime") or time_elem.get_text(strip=True)

        # ì½ëŠ” ì‹œê°„ (ë³´í†µ "X min read" íŒ¨í„´)
        for elem in soup.find_all(["span", "p", "div"]):
            text = elem.get_text(strip=True)
            if re.match(r"\d+\s*min\s*read", text, re.IGNORECASE):
                meta["read_time"] = text
                break

        return meta

    def _extract_scribe_body(self, soup: BeautifulSoup) -> str:
        """Scribe.ripì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""
        # article íƒœê·¸ ìš°ì„ 
        article_content = (
            soup.select_one("article")
            or soup.select_one("main")
            or soup.select_one(".content")
            or soup.select_one("body")
        )

        if not article_content:
            return ""

        content_parts = []
        seen_texts = set()  # ì¤‘ë³µ ì œê±°ìš©

        # ëª¨ë“  ì˜ë¯¸ ìˆëŠ” íƒœê·¸ ìˆœíšŒ
        tags = article_content.find_all(
            [
                "h1",
                "h2",
                "h3",
                "h4",
                "h5",
                "h6",
                "p",
                "blockquote",
                "pre",
                "ul",
                "ol",
                "figure",
                "img",
            ]
        )

        for tag in tags:
            text_content = self._format_tag(tag)
            if text_content and text_content not in seen_texts:
                content_parts.append(text_content)
                seen_texts.add(text_content)

        return "\n".join(content_parts)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # trafilatura Fallback
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _extract_with_trafilatura(self, url: str) -> CrawledArticle | None:
        """
        trafilaturaë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ Medium URLì—ì„œ ì§ì ‘ ì½˜í…ì¸  ì¶”ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.

        ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ê°€ ëª¨ë‘ ì‹¤íŒ¨í–ˆì„ ë•Œ fallbackìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        """
        try:
            html = await self.fetch_html(url)
            if not html:
                logger.warning(f"trafilatura: HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ - {url}")
                return None

            # trafilaturaë¡œ ë³¸ë¬¸ ì¶”ì¶œ
            content = trafilatura.extract(
                html,
                favor_recall=True,  # ë” ë§ì€ ì½˜í…ì¸  ì¶”ì¶œ ìš°ì„ 
                include_comments=False,
                include_tables=True,
            )

            if not content or len(content) < 100:
                logger.warning(f"trafilatura: ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¶ˆì¶©ë¶„ - {url}")
                return None

            logger.info(f"âœ… trafilatura ì„±ê³µ! ({len(content):,} ì)")

            # OG ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            soup = self.parse_html(html)
            og_meta = self.extract_og_meta(soup)

            # ì œëª© ê²°ì • (OG íƒœê·¸ ë˜ëŠ” title íƒœê·¸)
            title = og_meta.get("og_title")
            if not title:
                title_tag = soup.find("title")
                if title_tag:
                    title = self.text_extractor.clean_text(title_tag.get_text())

            # ArticleMetadata ìƒì„±
            metadata = self._build_metadata(og_meta)

            return CrawledArticle(
                title=title or "Medium Article",
                content=content,
                url=url,
                platform=self.platform_name,
                metadata=metadata,
                secondary_urls=[],
            )

        except Exception as e:
            logger.error(f"trafilatura extraction error: {e}")
            return None

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Playwright Fallback (ë™ì  ë Œë”ë§)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _extract_with_playwright(self, url: str) -> CrawledArticle | None:
        """
        Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ë™ì ìœ¼ë¡œ ë Œë”ë§ëœ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ëª¨ë“  ë¯¸ëŸ¬ ì„œë¹„ìŠ¤ì™€ trafilaturaê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ ìµœí›„ì˜ fallbackìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        - ì‹¤ì œ ë¸Œë¼ìš°ì € í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜
        - JavaScript ë Œë”ë§ ëŒ€ê¸°
        - ë´‡ íƒì§€ ìš°íšŒ ê°€ëŠ¥ì„± ë†’ìŒ
        """
        logger.info(f"ğŸ­ Playwright ë™ì  ë Œë”ë§ ì‹œë„: {url}")

        try:
            async with async_playwright() as p:
                # Chromium ë¸Œë¼ìš°ì € ì‹¤í–‰ (headless ëª¨ë“œ)
                browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        "--disable-blink-features=AutomationControlled",
                        "--disable-dev-shm-usage",
                        "--no-sandbox",
                    ],
                )

                # ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (ì‹¤ì œ ì‚¬ìš©ìì²˜ëŸ¼ ë³´ì´ê²Œ)
                context = await browser.new_context(
                    viewport={"width": 1920, "height": 1080},
                    user_agent=(
                        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/120.0.0.0 Safari/537.36"
                    ),
                    locale="ko-KR",
                    timezone_id="Asia/Seoul",
                )

                page = await context.new_page()

                # í˜ì´ì§€ ë¡œë“œ (ë„¤íŠ¸ì›Œí¬ ì•ˆì •í™”ê¹Œì§€ ëŒ€ê¸°)
                try:
                    await page.goto(url, wait_until="networkidle", timeout=30000)
                except PlaywrightTimeout:
                    logger.warning(
                        "Playwright: í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ, ë¶€ë¶„ ì½˜í…ì¸ ë¡œ ì§„í–‰..."
                    )

                # ì¶”ê°€ ëŒ€ê¸° (JavaScript ë Œë”ë§ ì™„ë£Œ)
                await page.wait_for_timeout(2000)

                # ìŠ¤í¬ë¡¤í•˜ì—¬ lazy-load ì½˜í…ì¸  ë¡œë“œ
                await page.evaluate(
                    "window.scrollTo(0, document.body.scrollHeight / 2)"
                )
                await page.wait_for_timeout(1000)

                # HTML ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
                html = await page.content()

                await browser.close()

                if not html or len(html) < 1000:
                    logger.warning("Playwright: HTML ì½˜í…ì¸  ë¶ˆì¶©ë¶„")
                    return None

                logger.info(f"Playwright HTML ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {len(html):,} bytes")

                # trafilaturaë¡œ ë³¸ë¬¸ ì¶”ì¶œ
                content = trafilatura.extract(
                    html,
                    favor_recall=True,
                    include_comments=False,
                    include_tables=True,
                )

                if not content or len(content) < 100:
                    # trafilatura ì‹¤íŒ¨ ì‹œ BeautifulSoup fallback
                    logger.info(
                        "Playwright: trafilatura ë¶ˆì¶©ë¶„, BeautifulSoup íŒŒì‹± ì‹œë„..."
                    )
                    soup = self.parse_html(html)
                    content = self._extract_medium_body(soup)

                if not content or len(content) < 100:
                    logger.warning("Playwright: ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨")
                    return None

                # 404 í˜ì´ì§€ ì½˜í…ì¸ ì¸ì§€ í™•ì¸
                if self._is_404_content(content):
                    logger.warning(
                        "Playwright: 404 í˜ì´ì§€ ì½˜í…ì¸  ê°ì§€, ìœ íš¨í•˜ì§€ ì•Šì€ ì•„í‹°í´"
                    )
                    return None

                logger.info(f"âœ… Playwright ì„±ê³µ! ({len(content):,} ì)")

                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                soup = self.parse_html(html)
                og_meta = self.extract_og_meta(soup)
                meta_info = self._extract_medium_metadata(soup)

                # ì œëª© ê²°ì •
                title = meta_info.get("title") or og_meta.get("og_title")
                if not title:
                    title_tag = soup.find("title")
                    if title_tag:
                        title = self.text_extractor.clean_text(title_tag.get_text())

                # ArticleMetadata ìƒì„±
                metadata = self._build_metadata(
                    og_meta,
                    author=meta_info.get("author"),
                    published_at=meta_info.get("date"),
                    read_time=meta_info.get("read_time"),
                    claps=meta_info.get("claps"),
                    tags=meta_info.get("tags"),
                    subtitle=meta_info.get("subtitle"),
                )

                return CrawledArticle(
                    title=title or "Medium Article",
                    content=content,
                    url=url,
                    platform=self.platform_name,
                    metadata=metadata,
                    secondary_urls=[],
                )

        except Exception as e:
            logger.error(f"Playwright extraction error: {e}")
            return None
